{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-4f53211e-ceab-5aa7-b0ba-6ef87e62c8ed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MIG-4f53211e-ceab-5aa7-b0ba-6ef87e62c8ed'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-4f53211e-ceab-5aa7-b0ba-6ef87e62c8ed\n",
    "%env CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.networks.nets import VNet\n",
    "\n",
    "# import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import cfg\n",
    "from dataset_mevis import MRI_dataset_batched\n",
    "from dsc import dice_coeff\n",
    "from funcs import *\n",
    "from models.sam import sam_model_registry, build_sam_mevis\n",
    "from models.sam.modeling.prompt_encoder import attention_fusion\n",
    "from predict_funs import *\n",
    "from utils import eval_seg, generate_click_prompt, iou_torch, pixel_accuracy\n",
    "\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20773273600, 20937965568)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.mem_get_info())\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2003\n",
    "IMGS_PATH = Path(\"/data/sab_data/images\")\n",
    "MASKS_PATH = Path(\"/data/sab_data/masks\")\n",
    "TRAIN_DATA_FILE = \"Train_data_files_resampled.json\"\n",
    "TEST_DATA_FILE =\"Test_data_files_resampled.json\"\n",
    "DEVICE = torch.device(\"cuda:\" + str(args.gpu_device))\n",
    "CHECKPOINT_DIRECTORY = Path(\"/data/sab_data/checkpoints\")\n",
    "LOGDIR = Path(\"/data/sab_data/model_logs\")\n",
    "sam_checkpoint = \"decoder_only/resampled/2024-08-12 10:30:07/mevis_sam-epoch60.pth\"\n",
    "# sam_checkpoint = \"bone_sam.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = cfg.parse_args()\n",
    "args.thd = True\n",
    "args.if_mask_decoder_adapter = True\n",
    "args.if_encoder_adapter = True\n",
    "args.decoder_adapt_depth = 2\n",
    "args.prompt_probability = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Thesis_code/models/sam/build_sam.py:343: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n",
      "/home/ubuntu/Thesis_code/models/sam/build_sam.py:353: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f, map_location=torch.device(device))\n"
     ]
    }
   ],
   "source": [
    "# V2\n",
    "def new_forward(\n",
    "    self,\n",
    "    batched_input: dict,\n",
    "    multimask_output: bool = True,\n",
    "    if_attention=False,\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Predicts masks end-to-end from provided images and prompts.\n",
    "    If prompts are not known in advance, using SamPredictor is\n",
    "    recommended over calling the model directly.\n",
    "\n",
    "    Arguments:\n",
    "        batched_input (list(dict)): A list over input images, each a\n",
    "        dictionary with the following keys. A prompt key can be\n",
    "        excluded if it is not present.\n",
    "            'image': The image as a torch tensor in 3xHxW format,\n",
    "            already transformed for input to the model.\n",
    "            'point_coords': (torch.Tensor) Batched point prompts for\n",
    "            this image, with shape BxNx2. Already transformed to the\n",
    "            input frame of the model.\n",
    "            'point_labels': (torch.Tensor) Batched labels for point prompts,\n",
    "            with shape BxN.\n",
    "            'boxes': (torch.Tensor) Batched box inputs, with shape Bx4.\n",
    "            Already transformed to the input frame of the model.\n",
    "            'mask_inputs': (torch.Tensor) Batched mask inputs to the model,\n",
    "            in the form Bx1xHxW.\n",
    "        multimask_output (bool): Whether the model should predict multiple\n",
    "        disambiguating masks, or return a single mask.\n",
    "\n",
    "    Returns:\n",
    "        (list(dict)): A list over input images, where each element is\n",
    "        as dictionary with the following keys.\n",
    "            'masks': (torch.Tensor) Batched binary mask predictions,\n",
    "            with shape BxCxHxW, where B is the number of input prompts,\n",
    "            C is determined by multimask_output, and (H, W) is the\n",
    "            original size of the image.\n",
    "    \"\"\"\n",
    "    image_embeddings = self.image_encoder(batched_input[\"image\"])\n",
    "    if if_attention:\n",
    "        image_embeddings = self.attention_fusion(\n",
    "            image_embeddings, batched_input[\"atten_maps\"].unsqueeze(1)\n",
    "        )\n",
    "\n",
    "    if \"point_coords\" in list(batched_input.keys()):\n",
    "        points = (batched_input[\"point_coords\"], batched_input[\"point_labels\"])\n",
    "    else:\n",
    "        points = None\n",
    "\n",
    "    sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "        points=points,\n",
    "        boxes=batched_input.get(\"boxes\", None),\n",
    "        masks=batched_input.get(\"mask_inputs\", None),\n",
    "    )\n",
    "    pe = torch.stack(\n",
    "        [self.prompt_encoder.get_dense_pe() for _ in range(image_embeddings.shape[0])],\n",
    "        dim=0,\n",
    "    ).squeeze(1)\n",
    "    low_res_masks, iou_predictions = self.mask_decoder(\n",
    "        image_embeddings=image_embeddings,\n",
    "        image_pe=pe,\n",
    "        sparse_prompt_embeddings=sparse_embeddings,\n",
    "        dense_prompt_embeddings=dense_embeddings,\n",
    "        multimask_output=multimask_output,\n",
    "    )\n",
    "\n",
    "    return low_res_masks\n",
    "\n",
    "\n",
    "sam_mevis = build_sam_mevis(\n",
    "    args,\n",
    "    checkpoint=CHECKPOINT_DIRECTORY / \"mobile_sam.pt\",\n",
    "    mevis_checkpoint=CHECKPOINT_DIRECTORY / sam_checkpoint,\n",
    "    num_classes=2,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "sam_mevis.forward = new_forward.__get__(sam_mevis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Thesis_code/models/sam/build_sam.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n",
      "/tmp/ipykernel_81661/1057709973.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    }
   ],
   "source": [
    "sam_mevis = sam_model_registry[\"vit_t\"](\n",
    "    args, checkpoint=CHECKPOINT_DIRECTORY / \"mobile_sam.pt\", num_classes=2\n",
    ")\n",
    "sam_mevis.attention_fusion = attention_fusion()\n",
    "sam_mevis.load_state_dict(\n",
    "    torch.load(\n",
    "        CHECKPOINT_DIRECTORY / sam_checkpoint,\n",
    "        map_location=torch.device(DEVICE),\n",
    "    ),\n",
    "    strict=True,\n",
    ")\n",
    "sam_mevis = sam_mevis.to(\"cpu\")#DEVICE)\n",
    "\n",
    "# vnet = VNet().to(DEVICE)\n",
    "# vnet.load_state_dict(\n",
    "#     torch.load(\n",
    "#         CHECKPOINT_DIRECTORY / \"atten.pth\",\n",
    "#         map_location=torch.device(DEVICE),\n",
    "#     )\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11492716\n"
     ]
    }
   ],
   "source": [
    "for param in sam_mevis.parameters():\n",
    "    param.requires_grad = True\n",
    "pytorch_total_params = sum(p.numel() for p in sam_mevis.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "132352\n"
     ]
    }
   ],
   "source": [
    "open_mask_decoder = [\n",
    "    \"transformer.layers.0.MLP_Adapter\" +\n",
    "    \"transformer.layers.0.Adapter\" +\n",
    "    \"transformer.layers.1.MLP_Adapter\" +\n",
    "    \"transformer.layers.1.Adapter\"\n",
    "]\n",
    "sam_mevis.eval()\n",
    "\n",
    "for param in sam_mevis.parameters():\n",
    "    param.requires_grad = False\n",
    "pytorch_total_params = sum(p.numel() for p in sam_mevis.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)\n",
    "\n",
    "for n, m in sam_mevis.mask_decoder.named_parameters():\n",
    "    if \"Adapter\" in n:\n",
    "        m.requires_grad = True\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in sam_mevis.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_seg(pred, true_mask_p, thresholds):\n",
    "    \"\"\"\n",
    "    threshold: a int or a tuple of ints\n",
    "    masks and pred: [b,1,h,w]\n",
    "    \"\"\"\n",
    "    results = {str(key): {} for key in thresholds}\n",
    "    b, c, h, w = pred.size()\n",
    "    if c == 2:\n",
    "        for th in thresholds:\n",
    "            iou_back, iou_class, dice_back, dice_class = 0, 0, 0, 0\n",
    "            vpred = (pred > th).to(torch.float32)\n",
    "            vpred_back = vpred[:, 0, :, :]\n",
    "            vpred_class = vpred[:, 1, :, :]\n",
    "\n",
    "            mask_back = true_mask_p[:, 0, :, :]\n",
    "            mask_class = true_mask_p[:, 1, :, :]\n",
    "\n",
    "            \"\"\"iou\"\"\"\n",
    "            iou_back = iou_torch(vpred_back.to(torch.int), mask_back.to(torch.int))\n",
    "            iou_class = iou_torch(vpred_class.to(torch.int), mask_class.to(torch.int))\n",
    "\n",
    "            \"\"\"dice for torch\"\"\"\n",
    "            dice_back = dice_coeff(vpred_back, mask_back).item()\n",
    "            dice_class = dice_coeff(vpred_class, mask_class).item()\n",
    "            results[str(th)] = {\n",
    "                \"iou_back\": iou_back,\n",
    "                \"iou_class\": iou_class,\n",
    "                \"dice_back\": dice_back,\n",
    "                \"dice_class\": dice_class,\n",
    "            }\n",
    "\n",
    "        return results\n",
    "    else:\n",
    "        for th in thresholds:\n",
    "\n",
    "            vpred = (pred > th).to(torch.float32)\n",
    "            iou_class = iou_torch(\n",
    "                vpred[:, 0, :, :].to(torch.int), true_mask_p[:, 0, :, :].to(torch.int)\n",
    "            )\n",
    "            dice_class = dice_coeff(vpred[:, 0, :, :], true_mask_p[:, 0, :, :]).item()\n",
    "            results[str(th)] = {\n",
    "                \"iou_class\": iou_class,\n",
    "                \"dice_class\": dice_class,\n",
    "            }\n",
    "\n",
    "        return results\n",
    "\n",
    "WARMUP_EPOCHS = 30\n",
    "BASE_LEARNING_RATE = 1e-4\n",
    "WARMUP_LR_START = 1e-5\n",
    "END_LEARNING_RATE = 1e-5\n",
    "\n",
    "def lr_scheduler(\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    n_warmup_epochs,\n",
    "    n_epochs,\n",
    "    base_lr=BASE_LEARNING_RATE,\n",
    "    warmup_lr_start=WARMUP_LR_START,\n",
    "    end_lr=END_LEARNING_RATE,\n",
    "):\n",
    "    if epoch <= n_warmup_epochs:\n",
    "        # Linear warmup phase\n",
    "        lr = warmup_lr_start + (base_lr - warmup_lr_start) * (epoch / n_warmup_epochs)\n",
    "    else:\n",
    "        # Linear decay phase\n",
    "        decay_epochs = n_epochs + n_warmup_epochs\n",
    "        lr = base_lr - (base_lr - end_lr) * (epoch / decay_epochs)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def write_hists(net, writer, epoch):\n",
    "    writer.add_histogram(\n",
    "        \"mask_decoder.transformer.layers[1].MLP_Adapter/D_fc1/weight\",\n",
    "        net.mask_decoder.transformer.layers[1].MLP_Adapter.D_fc1.weight,\n",
    "        epoch,\n",
    "    )\n",
    "    writer.add_histogram(\n",
    "        \"mask_decoder.transformer.layers[1].MLP_Adapter/D_fc1/bias\",\n",
    "        net.mask_decoder.transformer.layers[1].MLP_Adapter.D_fc1.bias,\n",
    "        epoch,\n",
    "    )\n",
    "\n",
    "    writer.add_histogram(\n",
    "        \"mask_decoder.transformer.layers[1].MLP_Adapter/D_fc2/weight\",\n",
    "        net.mask_decoder.transformer.layers[1].MLP_Adapter.D_fc2.weight,\n",
    "        epoch,\n",
    "    )\n",
    "    writer.add_histogram(\n",
    "        \"mask_decoder.transformer.layers[1].MLP_Adapter/D_fc2/bias\",\n",
    "        net.mask_decoder.transformer.layers[1].MLP_Adapter.D_fc2.bias,\n",
    "        epoch,\n",
    "    )\n",
    "\n",
    "    writer.add_histogram(\n",
    "        \"mask_decoder.transformer.layers[1].Adapter/D_fc1/weight\",\n",
    "        net.mask_decoder.transformer.layers[1].Adapter.D_fc1.weight,\n",
    "        epoch,\n",
    "    )\n",
    "    writer.add_histogram(\n",
    "        \"mask_decoder.transformer.layers[1].Adapter/D_fc1/bias\",\n",
    "        net.mask_decoder.transformer.layers[1].Adapter.D_fc1.bias,\n",
    "        epoch,\n",
    "    )\n",
    "\n",
    "    writer.add_histogram(\n",
    "        \"mask_decoder.transformer.layers[1].Adapter/D_fc2/weight\",\n",
    "        net.mask_decoder.transformer.layers[1].Adapter.D_fc2.weight,\n",
    "        epoch,\n",
    "    )\n",
    "    writer.add_histogram(\n",
    "        \"mask_decoder.transformer.layers[1].Adapter/D_fc2/bias\",\n",
    "        net.mask_decoder.transformer.layers[1].Adapter.D_fc2.bias,\n",
    "        epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def lr_num(\n",
    "    epoch,\n",
    "    n_warmup_epochs=30,\n",
    "    n_epochs=90,\n",
    "    base_lr=1e-4,\n",
    "    warmup_lr_start=1e-5,\n",
    "    end_lr=1e-5,\n",
    "):\n",
    "    if epoch <= n_warmup_epochs:\n",
    "        # Linear warmup phase\n",
    "        lr = warmup_lr_start + (base_lr - warmup_lr_start) * (epoch / n_warmup_epochs)\n",
    "    else:\n",
    "        # Linear decay phase\n",
    "        total_decay_epochs = n_epochs - n_warmup_epochs\n",
    "        decay_epoch = epoch - n_warmup_epochs\n",
    "        lr = base_lr - (base_lr - end_lr) * (decay_epoch / total_decay_epochs)\n",
    "    return lr\n",
    "\n",
    "\n",
    "x = list(range(90))\n",
    "lr_out = [lr_num(i) for i in x]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x, lr_out, marker=\"o\", linestyle=\"-\", markersize=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mevis(\n",
    "    args,\n",
    "    net: torch.nn.Module,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    dataset,\n",
    "    epoch,\n",
    "    writer,\n",
    "):\n",
    "    epoch_loss = 0\n",
    "    # train mode\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with tqdm(total=len(dataset), desc=f\"Epoch {epoch}\", unit=\"batch\") as pbar:\n",
    "        for i in range(len(dataset)):\n",
    "            pack = dataset[i]\n",
    "            imgs = pack[\"images\"]\n",
    "            masks = pack[\"masks\"]\n",
    "            attens = pack[\"atten_map\"]\n",
    "            # Send prompt with the probablity of *args.prompt_probability*, else None\n",
    "            if torch.bernoulli(torch.tensor([args.prompt_probability])).bool().item():\n",
    "                if \"pt\" not in pack:\n",
    "                    imgs, pt, masks = generate_click_prompt(imgs, masks)\n",
    "                    point_labels = torch.ones(imgs.size(0))\n",
    "                else:\n",
    "                    pt = pack[\"pt\"]\n",
    "                    point_labels = pack[\"p_label\"]\n",
    "\n",
    "                if point_labels[0] != -1:\n",
    "                    point_coords = pt\n",
    "                    coords_torch = torch.as_tensor(\n",
    "                        point_coords, dtype=torch.float, device=DEVICE\n",
    "                    )\n",
    "                    labels_torch = torch.as_tensor(\n",
    "                        point_labels, dtype=torch.int, device=DEVICE\n",
    "                    )\n",
    "                    coords_torch, labels_torch = (\n",
    "                        coords_torch[None, :, :],\n",
    "                        labels_torch[None, :],\n",
    "                    )\n",
    "                    pt = (coords_torch, labels_torch)\n",
    "\n",
    "            else:\n",
    "                pt = None\n",
    "\n",
    "            \"\"\"Train\"\"\"\n",
    "            with torch.no_grad():\n",
    "                imge = net.image_encoder(imgs)\n",
    "                se, de = net.prompt_encoder(\n",
    "                    points=pt,\n",
    "                    boxes=None,\n",
    "                    masks=None,\n",
    "                )\n",
    "                pe = torch.stack(\n",
    "                    [net.prompt_encoder.get_dense_pe() for _ in range(imge.shape[0])],\n",
    "                    dim=0,\n",
    "                ).squeeze(1)\n",
    "                imge = net.attention_fusion(imge, attens.unsqueeze(1))\n",
    "            pred, _ = net.mask_decoder(\n",
    "                image_embeddings=imge,\n",
    "                image_pe=pe,\n",
    "                sparse_prompt_embeddings=se,\n",
    "                dense_prompt_embeddings=de,\n",
    "                multimask_output=True,\n",
    "            )\n",
    "\n",
    "            loss = loss_func(pred[:, 1:2, :, :], masks)\n",
    "            pbar.set_postfix(**{\"loss (batch)\": loss.item()})\n",
    "            writer.add_scalar(\n",
    "                \"Batch Loss/Training\", loss.item(), epoch * len(dataset) + i\n",
    "            )\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def validate_mevis(\n",
    "    args,\n",
    "    net: torch.nn.Module,\n",
    "    loss_func,\n",
    "    dataset,\n",
    "    epoch,\n",
    "    writer,\n",
    "):\n",
    "    net.eval()\n",
    "    sum_loss = 0\n",
    "    thresholds = (0.0, 0.25, 0.5, 0.7, 0.9)\n",
    "    result_dict = {str(key): {\"iou_class\": 0, \"dice_class\": 0} for key in thresholds}\n",
    "    n_val = len(dataset)\n",
    "\n",
    "    with tqdm(total=n_val, desc=\"Validation round\", unit=\"batch\", leave=True) as pbar:\n",
    "        for i in range(n_val):\n",
    "            pack = dataset[i]\n",
    "            imgsw = pack[\"images\"]\n",
    "            masksw = pack[\"masks\"]\n",
    "            attensw = pack[\"atten_map\"]\n",
    "            \n",
    "            # Send prompt with the probablity of *args.prompt_probability*, else None\n",
    "            if torch.bernoulli(torch.tensor([args.prompt_probability])).bool().item():\n",
    "                if \"pt\" not in pack:\n",
    "                    imgsw, ptw, masksw = generate_click_prompt(imgsw, masksw)\n",
    "                else:\n",
    "                    ptw = pack[\"pt\"]\n",
    "                    point_labels = pack[\"p_label\"]\n",
    "            else:\n",
    "                ptw = None\n",
    "\n",
    "            buoy = 0\n",
    "            if args.evl_chunk:\n",
    "                evl_ch = int(args.evl_chunk)\n",
    "            else:\n",
    "                evl_ch = int(imgsw.size(0))\n",
    "\n",
    "            while (buoy + evl_ch) <= imgsw.size(0):\n",
    "                pt = ptw\n",
    "                imgs = imgsw[buoy : buoy + evl_ch]\n",
    "                masks = masksw[buoy : buoy + evl_ch]\n",
    "                attens = attensw[buoy : buoy + evl_ch]\n",
    "                buoy += evl_ch\n",
    "\n",
    "                point_labels = torch.ones(imgs.size(0))\n",
    "\n",
    "                if point_labels[0] != -1 and pt != None:\n",
    "                    point_coords = pt\n",
    "                    coords_torch = torch.as_tensor(\n",
    "                        point_coords, dtype=torch.float, device=DEVICE\n",
    "                    )\n",
    "                    labels_torch = torch.as_tensor(\n",
    "                        point_labels, dtype=torch.int, device=DEVICE\n",
    "                    )\n",
    "                    coords_torch, labels_torch = (\n",
    "                        coords_torch[None, :, :],\n",
    "                        labels_torch[None, :],\n",
    "                    )\n",
    "                    pt = (coords_torch, labels_torch)\n",
    "\n",
    "                \"\"\"test\"\"\"\n",
    "                with torch.no_grad():\n",
    "                    imge = net.image_encoder(imgs)\n",
    "\n",
    "                    se, de = net.prompt_encoder(\n",
    "                        points=pt,\n",
    "                        boxes=None,\n",
    "                        masks=None,\n",
    "                    )\n",
    "                    pe = torch.stack(\n",
    "                        [\n",
    "                            net.prompt_encoder.get_dense_pe()\n",
    "                            for _ in range(imge.shape[0])\n",
    "                        ],\n",
    "                        dim=0,\n",
    "                    ).squeeze(1)\n",
    "                    imge = net.attention_fusion(imge, attens.unsqueeze(1))\n",
    "                    pred, _ = net.mask_decoder(\n",
    "                        image_embeddings=imge,\n",
    "                        image_pe=pe,\n",
    "                        sparse_prompt_embeddings=se,\n",
    "                        dense_prompt_embeddings=de,\n",
    "                        multimask_output=True,\n",
    "                    )\n",
    "\n",
    "                    loss = loss_func(pred[:, 1:, :, :], masks)\n",
    "                    pbar.set_postfix(**{\"loss (batch)\": loss.item()})\n",
    "                    # writer.add_scalar(\n",
    "                    #     \"Batch Loss/validation\", loss.item(), epoch * len(dataset) + i\n",
    "                    # )\n",
    "                    sum_loss += loss\n",
    "                    temp = eval_seg(pred[:, 1:, :, :], masks, thresholds)\n",
    "                    for th in result_dict.keys():\n",
    "                        for met, val in result_dict[th].items():\n",
    "                            result_dict[th][met] = val + temp[th].get(met, 0)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    if args.evl_chunk:\n",
    "        n_val = n_val * (imgsw.size(0) // evl_ch)\n",
    "    for th in result_dict.keys():\n",
    "        result_dict[th] = {k: val / n_val for k, val in result_dict[th].items()}\n",
    "\n",
    "    return sum_loss / n_val, result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_BATCH_SIZE = 20\n",
    "\n",
    "dataset_test = MRI_dataset_batched(\n",
    "    args,\n",
    "    data_file=TEST_DATA_FILE,\n",
    "    batch_size=DATASET_BATCH_SIZE,\n",
    "    phase=\"test\",\n",
    "    operation_mode=\"queue\",\n",
    "    mask_out_size=args.out_size,\n",
    "    attention_size=64,\n",
    "    channel_num=1,\n",
    "    crop=False,\n",
    "    crop_size=1024,\n",
    "    targets=[\"all\"],\n",
    "    cls=1,\n",
    "    if_prompt=True,\n",
    "    prompt_type=\"point\",\n",
    "    region_type=\"largest_15\",\n",
    "    prompt_num=15,\n",
    "    if_attention_map=True,\n",
    "    device=\"cpu\",#DEVICE,\n",
    ")\n",
    "\n",
    "dataset_train = MRI_dataset_batched(\n",
    "    args,\n",
    "    data_file=TRAIN_DATA_FILE,\n",
    "    batch_size=DATASET_BATCH_SIZE,\n",
    "    phase=\"train\",\n",
    "    operation_mode=\"queue\",\n",
    "    mask_out_size=args.out_size,\n",
    "    attention_size=64,\n",
    "    channel_num=1,\n",
    "    crop=False,\n",
    "    crop_size=1024,\n",
    "    targets=[\"all\"],\n",
    "    cls=1,\n",
    "    if_prompt=True,\n",
    "    prompt_type=\"point\",\n",
    "    region_type=\"largest_15\",\n",
    "    prompt_num=15,\n",
    "    if_attention_map=True,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.backends.cudnn.benchmark = True\n",
    "BASE_LEARNING_RATE = 5e-4\n",
    "WARMUP_LR_START = 1E-5\n",
    "lossfunc = DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "optimizer = torch.optim.AdamW(sam_mevis.parameters(), lr=BASE_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 132352\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "WARMUP_EPOCHS = 20\n",
    "valid_every = 2\n",
    "save_every = 3\n",
    "train_folder = \"decoder_only/resampled\"\n",
    "base_save = CHECKPOINT_DIRECTORY / train_folder\n",
    "base_save.mkdir(exist_ok=True)\n",
    "log_save = LOGDIR / train_folder\n",
    "log_save.mkdir(parents=True, exist_ok=True)\n",
    "if \"bone_sam\" in sam_checkpoint:\n",
    "    start_time = datetime.now().strftime(r\"%Y-%m-%d %H:%M:%S\")\n",
    "    epoch = 0\n",
    "else:\n",
    "    start_time = sam_checkpoint.split(\"/\")[1]\n",
    "    epoch = int(sam_checkpoint.split(\"epoch\")[1].split(\"-\")[0])\n",
    "writer = SummaryWriter(log_dir=log_save / start_time)\n",
    "\n",
    "epoch = 0\n",
    "pytorch_total_params = sum(p.numel() for p in sam_mevis.parameters() if p.requires_grad)\n",
    "print(\"Number of trainable parameters:\", pytorch_total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=256, out_features=64, bias=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_mevis.mask_decoder.transformer.layers[1].Adapter.D_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cec9f7a7ac49e880397fa1900633e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/250 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f284f7ca9800482ea47ed4535f9d3cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation round:   0%|          | 0/66 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model at /data/sab_data/checkpoints/decoder_only/mevis_sam-epoch0.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35d78fc8aeb4033bcafc6ab4f4ede32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/250 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdd947057af4b68b96e650e46979f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/250 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5629da2ce3324344862eccbc42ceb57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation round:   0%|          | 0/66 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de71ce463f8048a9b8d1cd38dac26cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/250 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model at /data/sab_data/checkpoints/decoder_only/mevis_sam-epoch3.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b1afe0436845fbacd9aea54d69d490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/250 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aff963ef9294d9daf142ad6194e0afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation round:   0%|          | 0/66 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587b05240bb0461caca0b3f1644f5404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/250 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb6b66aab8c4b24b9877ceca3698110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/250 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3428ea888e45f48fe50a5da8943b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation round:   0%|          | 0/66 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model at /data/sab_data/checkpoints/decoder_only/mevis_sam-epoch6.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244020107a934a18b607f08397676168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/250 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a003a1e5c7cf4b10af6863d1b7a228da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/250 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a93320489c4fc18cd287c8532b606c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation round:   0%|          | 0/66 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfef86a45a1485fa8dd930815fe89e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/250 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model at /data/sab_data/checkpoints/decoder_only/mevis_sam-epoch9.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54717ba65f9f4855b153172b4fd3f261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/250 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3171b9092b47deba2a8c00eecfd939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation round:   0%|          | 0/66 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m write_hists(sam_mevis, writer\u001b[38;5;241m=\u001b[39mwriter, epoch\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m valid_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     val_loss, val_result \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_mevis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msam_mevis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlossfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalars(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiceCEloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_loss}, epoch)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m val_result\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[9], line 130\u001b[0m, in \u001b[0;36mvalidate_mevis\u001b[0;34m(args, net, loss_func, dataset, epoch, writer)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mn_val, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation round\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_val):\n\u001b[0;32m--> 130\u001b[0m         pack \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    131\u001b[0m         imgsw \u001b[38;5;241m=\u001b[39m pack[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    132\u001b[0m         masksw \u001b[38;5;241m=\u001b[39m pack[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Thesis_code/dataset_mevis.py:398\u001b[0m, in \u001b[0;36mMRI_dataset_batched.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueue\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 398\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_que_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    400\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_list[index]]\n",
      "File \u001b[0;32m~/Thesis_code/dataset_mevis.py:181\u001b[0m, in \u001b[0;36mMRI_dataset_batched.get_que_batch\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmri index overshoot, calling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m strat \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, end \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, toall \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m     )\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vol \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(volume_data):\n\u001b[0;32m--> 181\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_volume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    183\u001b[0m         loaded_batch \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/Thesis_code/dataset_mevis.py:222\u001b[0m, in \u001b[0;36mMRI_dataset_batched._process_volume\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img_path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m    219\u001b[0m     img_vol \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    220\u001b[0m         sitk\u001b[38;5;241m.\u001b[39mGetArrayFromImage(sitk\u001b[38;5;241m.\u001b[39mReadImage(img_path)), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 222\u001b[0m     img_vol \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_vol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_vol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_vol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_vol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading the image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while epoch < (WARMUP_EPOCHS + EPOCHS):\n",
    "    optimizer = lr_scheduler(\n",
    "        optimizer,\n",
    "        epoch,\n",
    "        warmup_epochs=WARMUP_EPOCHS,\n",
    "        base_lr=BASE_LEARNING_RATE,\n",
    "        warmup_lr_start=WARMUP_LR_START,\n",
    "    )\n",
    "    dataset_train.on_epoch_end()\n",
    "    loss = train_mevis(\n",
    "        args,\n",
    "        net=sam_mevis,\n",
    "        optimizer=optimizer,\n",
    "        loss_func=lossfunc,\n",
    "        dataset=dataset_train,\n",
    "        epoch=epoch,\n",
    "        writer=writer,\n",
    "    )\n",
    "    writer.add_scalars(\"DiceCEloss\", {\"training\": loss}, epoch)\n",
    "    write_hists(sam_mevis, writer=writer, epoch=epoch)\n",
    "    if epoch % valid_every == 0:\n",
    "        val_loss, val_result = validate_mevis(\n",
    "            args,\n",
    "            net=sam_mevis,\n",
    "            loss_func=lossfunc,\n",
    "            dataset=dataset_test,\n",
    "            epoch=epoch,\n",
    "            writer=writer,\n",
    "        )\n",
    "        writer.add_scalars(\"DiceCEloss\", {\"validation\": val_loss}, epoch)\n",
    "        for key, value in val_result.items():\n",
    "            for key_in, value_in in value.items():\n",
    "                writer.add_scalar(f\"Validation Metrics/{key_in}/thr{key}\", value_in, epoch)\n",
    "    if epoch % save_every == 0:\n",
    "        save_path = base_save / f\"mevis_sam-epoch{epoch}.pth\"\n",
    "        torch.save(sam_mevis.state_dict(), save_path)\n",
    "        print(\n",
    "            f\"saved model at {str(save_path)}\"\n",
    "        )\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_hists(sam_mevis, writer=writer, epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2434391cc848da8274fa2a5dd2280a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation round:   0%|          | 0/66 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_loss, val_result = validate_mevis(\n",
    "            args,\n",
    "            net=sam_mevis,\n",
    "            loss_func=lossfunc,\n",
    "            dataset=dataset_test,\n",
    "            epoch=1,\n",
    "            writer=writer,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in val_result.items():\n",
    "    for key_in, value_in in value.items():\n",
    "        writer.add_scalar(f\"IOU/{key_in}/{key}\", value_in, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5037, device='cuda:0') {'0.0': {'iou_back': -1.0, 'iou_class': 0.5267612152930462, 'dice_back': -1.0, 'dice_class': 0.5173862292007967}, '0.25': {'iou_back': -1.0, 'iou_class': 0.5405494137243791, 'dice_back': -1.0, 'dice_class': 0.5240659442814913}, '0.5': {'iou_back': -1.0, 'iou_class': 0.5499592344417716, 'dice_back': -1.0, 'dice_class': 0.5281071554530751}, '0.7': {'iou_back': -1.0, 'iou_class': 0.5536853079542969, 'dice_back': -1.0, 'dice_class': 0.5287035708174561}, '0.9': {'iou_back': -1.0, 'iou_class': 0.5542145951679258, 'dice_back': -1.0, 'dice_class': 0.5272461731325496}}\n"
     ]
    }
   ],
   "source": [
    "print(loss, val_loss, val_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for base model on test set:\n",
    "\n",
    "    iou               0.5267612152930462\n",
    "    dice_score        0.5173862292007967\n",
    "\\\n",
    "decoder_only/2024-08-10 19:26:43/mevis_sam-epoch80-final.pth model on test set:\n",
    "\n",
    "    pixel_accuracy    0.985118\n",
    "    iou                0.60849\n",
    "    dice_score        0.617276\n",
    "    DiceCELoss        1.539488\n",
    "\\\n",
    "decoder_only/2024-08-10 19:26:43/mevis_sam-epoch60-final.pth model on test set:\n",
    "\n",
    "    pixel_accuracy    0.985452\n",
    "    iou               0.606201\n",
    "    dice_score        0.620731\n",
    "    DiceCELoss        1.539338\n",
    "\\\n",
    "\n",
    "decoder_only/resampled/2024-08-12 10:30:07/mevis_sam-epoch60.pth model on test set:\n",
    "\n",
    "    pixel_accuracy    0.983529\n",
    "    iou               0.633876\n",
    "    dice_score        0.656472\n",
    "    DiceCELoss        1.519845\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers to train:\n",
    "* Image encoder:\n",
    "    + patch_embed\n",
    "    + layers.1.blocks.0.MLP_Adapter\n",
    "    + layers.1.blocks.0.Space_Adapter\n",
    "    + layers.1.blocks.1.MLP_Adapter\n",
    "    + layers.1.blocks.1.Space_Adapter\n",
    "    + layers.2.blocks.0.MLP_Adapter\n",
    "    + layers.2.blocks.0.Space_Adapter\n",
    "    + layers.2.blocks.1.MLP_Adapter\n",
    "    + layers.2.blocks.1.Space_Adapter\n",
    "* Mask decoder:\n",
    "    + transformer.layers.0.MLP_Adapter\n",
    "    + transformer.layers.0.Adapter\n",
    "    + transformer.layers.1.MLP_Adapter\n",
    "    + transformer.layers.1.Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_layer_names(model):\n",
    "    unique_layer_names = set()\n",
    "    for module in model.modules():\n",
    "        name = module.__class__.__name__\n",
    "        unique_layer_names.add(name)\n",
    "    return list(unique_layer_names)\n",
    "\n",
    "adapter_layers = get_unique_layer_names(sam_mevis.mask_decoder)\n",
    "adapter_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image encoder: patch_embed\n",
      "PatchEmbed(\n",
      "  (seq): Sequential(\n",
      "    (0): Conv2d_BN(\n",
      "      (c): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Conv2d_BN(\n",
      "      (c): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Image encoder: layers.1.blocks.0.MLP_Adapter\n",
      "Adapter(\n",
      "  (act): GELU(approximate='none')\n",
      "  (D_fc1): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (D_fc2): Linear(in_features=32, out_features=128, bias=True)\n",
      ")\n",
      "Image encoder: layers.1.blocks.0.Space_Adapter\n",
      "Adapter(\n",
      "  (act): GELU(approximate='none')\n",
      "  (D_fc1): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (D_fc2): Linear(in_features=32, out_features=128, bias=True)\n",
      ")\n",
      "Image encoder: layers.1.blocks.1.MLP_Adapter\n",
      "Adapter(\n",
      "  (act): GELU(approximate='none')\n",
      "  (D_fc1): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (D_fc2): Linear(in_features=32, out_features=128, bias=True)\n",
      ")\n",
      "Image encoder: layers.1.blocks.1.Space_Adapter\n",
      "Adapter(\n",
      "  (act): GELU(approximate='none')\n",
      "  (D_fc1): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (D_fc2): Linear(in_features=32, out_features=128, bias=True)\n",
      ")\n",
      "Image encoder: layers.2.blocks.0.MLP_Adapter\n",
      "Adapter(\n",
      "  (act): GELU(approximate='none')\n",
      "  (D_fc1): Linear(in_features=160, out_features=40, bias=True)\n",
      "  (D_fc2): Linear(in_features=40, out_features=160, bias=True)\n",
      ")\n",
      "Image encoder: layers.2.blocks.0.Space_Adapter\n",
      "Adapter(\n",
      "  (act): GELU(approximate='none')\n",
      "  (D_fc1): Linear(in_features=160, out_features=40, bias=True)\n",
      "  (D_fc2): Linear(in_features=40, out_features=160, bias=True)\n",
      ")\n",
      "Image encoder: layers.2.blocks.1.MLP_Adapter\n",
      "Adapter(\n",
      "  (act): GELU(approximate='none')\n",
      "  (D_fc1): Linear(in_features=160, out_features=40, bias=True)\n",
      "  (D_fc2): Linear(in_features=40, out_features=160, bias=True)\n",
      ")\n",
      "Image encoder: layers.2.blocks.1.Space_Adapter\n",
      "Adapter(\n",
      "  (act): GELU(approximate='none')\n",
      "  (D_fc1): Linear(in_features=160, out_features=40, bias=True)\n",
      "  (D_fc2): Linear(in_features=40, out_features=160, bias=True)\n",
      ")\n",
      "Mask decoder: transformer.layers.0.MLP_Adapter\n",
      "Adapter(\n",
      "  (act): GELU(approximate='none')\n",
      "  (D_fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (D_fc2): Linear(in_features=64, out_features=256, bias=True)\n",
      ")\n",
      "Mask decoder: transformer.layers.0.Adapter\n",
      "Adapter(\n",
      "  (act): GELU(approximate='none')\n",
      "  (D_fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (D_fc2): Linear(in_features=64, out_features=256, bias=True)\n",
      ")\n",
      "Mask decoder: transformer.layers.1.MLP_Adapter\n",
      "Adapter(\n",
      "  (act): GELU(approximate='none')\n",
      "  (D_fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (D_fc2): Linear(in_features=64, out_features=256, bias=True)\n",
      ")\n",
      "Mask decoder: transformer.layers.1.Adapter\n",
      "Adapter(\n",
      "  (act): GELU(approximate='none')\n",
      "  (D_fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (D_fc2): Linear(in_features=64, out_features=256, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "image_encoder_layers = [\n",
    "    \"patch_embed\",\n",
    "    \"layers.1.blocks.0.MLP_Adapter\",\n",
    "    \"layers.1.blocks.0.Space_Adapter\",\n",
    "    \"layers.1.blocks.1.MLP_Adapter\",\n",
    "    \"layers.1.blocks.1.Space_Adapter\",\n",
    "    \"layers.2.blocks.0.MLP_Adapter\",\n",
    "    \"layers.2.blocks.0.Space_Adapter\",\n",
    "    \"layers.2.blocks.1.MLP_Adapter\",\n",
    "    \"layers.2.blocks.1.Space_Adapter\",\n",
    "]\n",
    "mask_decoder_layers = [\n",
    "    \"transformer.layers.0.MLP_Adapter\",\n",
    "    \"transformer.layers.0.Adapter\",\n",
    "    \"transformer.layers.1.MLP_Adapter\",\n",
    "    \"transformer.layers.1.Adapter\",\n",
    "]\n",
    "for n, m in list(sam_mevis.image_encoder.named_modules()) + list(sam_mevis.mask_decoder.named_modules()):\n",
    "    if n in image_encoder_layers:\n",
    "        print(\"Image encoder:\", n)\n",
    "        print(m)\n",
    "    if n in mask_decoder_layers:\n",
    "        print(\"Mask decoder:\", n)\n",
    "        print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1168e23ac51640d78ad4117971f52150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def eval_model(net, dataset, loss_func):\n",
    "    net.eval()\n",
    "    result_dict = {}\n",
    "    with tqdm(\n",
    "        total=len(dataset),\n",
    "        desc=f\"Running inference on queued batches of size {dataset.batch_size}\",\n",
    "        unit=\"Batch\",\n",
    "    ) as pbar:\n",
    "        for ind in range(len(dataset)):\n",
    "            data = dataset[ind]\n",
    "            low_res_masks, _ = do_inference_on_batch(net, data)\n",
    "            low_res_masks_bool = low_res_masks > net.mask_threshold\n",
    "            cat_indexes = data.get(\"cat_indexes\", [0])\n",
    "            for i in range(len(cat_indexes)):\n",
    "                postfix = \"\" if i == 0 else str(i)\n",
    "                orig_size = data[f\"original_size{postfix}\"]\n",
    "                img_name = data[f\"img_name{postfix}\"]\n",
    "                slices = data[f\"slices{postfix}\"]\n",
    "\n",
    "                if i + 1 >= len(cat_indexes):\n",
    "                    sliced_mask = data[\"masks\"][cat_indexes[i] :]\n",
    "                    sliced_prediction = low_res_masks_bool[cat_indexes[i] :]\n",
    "                else:\n",
    "                    sliced_mask = data[\"masks\"][cat_indexes[i] : cat_indexes[i + 1]]\n",
    "                    sliced_prediction = low_res_masks_bool[\n",
    "                        cat_indexes[i] : cat_indexes[i + 1]\n",
    "                    ]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    dice_score_batch = dice_coeff(\n",
    "                        sliced_mask, sliced_prediction.to(torch.float32)[:, 1:2, :, :]\n",
    "                    ).item()\n",
    "\n",
    "                pixel_acc_batch = pixel_accuracy(\n",
    "                    sliced_prediction[:, 1:2, :, :], sliced_mask\n",
    "                )\n",
    "                iou_batch = iou_torch(\n",
    "                    sliced_prediction[:, 1:2, :, :], sliced_mask.clone().to(torch.int)\n",
    "                )\n",
    "\n",
    "                loss = loss_func(sliced_prediction[:, 1:2, :, :].to(torch.float32), sliced_mask)\n",
    "                # temp = eval_seg(pred[:,1:, :, :], masks, thresholds)\n",
    "                # print(f\"temp at epoch {epoch}: {temp} len:{pred.size(0)}\")\n",
    "                # for th in result_dict.keys():\n",
    "                #     for met, val in result_dict[th].items():\n",
    "                #         result_dict[th][met] = val + temp[th][met]\n",
    "\n",
    "                if img_name in result_dict.keys():\n",
    "                    result_dict[img_name][\"prediction_path\"].append(f\"{img_name.split(\".\")[0]}-{slices[0]}-{slices[-1]}\")\n",
    "                    result_dict[img_name][\"pixel_accuracy\"].append(pixel_acc_batch)\n",
    "                    result_dict[img_name][\"iou\"].append(iou_batch)\n",
    "                    result_dict[img_name][\"dice_score\"].append(dice_score_batch)\n",
    "                    result_dict[img_name][\"DiceCELoss\"].append(loss.item())\n",
    "                else:\n",
    "                    result_dict[img_name] = {\n",
    "                        \"prediction_path\": [f\"{img_name.split(\".\")[0]}-{slices[0]}-{slices[-1]}\"],\n",
    "                        \"pixel_accuracy\": [pixel_acc_batch],\n",
    "                        \"iou\": [iou_batch],\n",
    "                        \"dice_score\": [dice_score_batch],\n",
    "                        \"DiceCELoss\": [loss.item()],\n",
    "                    }\n",
    "            pbar.update(1)\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "result_dict_eval = eval_model(sam_mevis, dataset_test, loss_func=lossfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_eval_merged = merge_metrics(result_dict_eval)\n",
    "\n",
    "# result_dict_eval_merged\n",
    "result_dict_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./eval_results/decoder_only_resampled_2024-08-12 10:30:07_epoch60_model_test.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        result_dict_eval_merged,\n",
    "        f,\n",
    "        indent=4,\n",
    "        sort_keys=False,\n",
    "        separators=(\",\", \": \"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pixel_accuracy    0.983529\n",
       "iou               0.633876\n",
       "dice_score        0.656472\n",
       "DiceCELoss        1.519845\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"pixel_accuracy    0.985118\n",
    "iou                0.60849\n",
    "dice_score        0.617276\n",
    "DiceCELoss        1.539488\n",
    "dtype: object\n",
    "\"\"\"\n",
    "df = pd.DataFrame.from_dict(result_dict_eval_merged).T.drop(\"prediction_path\", axis=1)\n",
    "df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_366368/3707489999.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0a9fc5e2c9460bb12fd35636b37517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fed402b4714419d9a5b1615124ffee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405fb1c26a7a4a8883474ced167b867d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edd298f89f2414ab869cabad9b0d00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc39b031c1342f78d0cbae104df5dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa722373e005484eb08967a042223a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7e3ae8fcba4716ac47a93129aa5312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f961224b1c4fc386d601c831a259fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e639368accf4850843c4e4afe626636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add101c631a344888e1be4992b181758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd8053305c54e6599147bcd0d2f11b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb7a5c821984f699e4f1d498be5cdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3162e5a3f9564f04aa8926ee953cdb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabf1c800fc2473d8378b5a9c2230e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4dbc5b2be24b388f5f1c8bee3c05df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3634c6e29af41a7bc793b584dc7fba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25b33bf6f50430c8974c8402e8047d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d566e7f6f0e41358de3838495260dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8f7c6fd1324df1b4b8f5128d10f294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3afb3696534ec48041c73e0d1ad2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1bea31b69246dc9d2ff44426b283f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2305e5d05cc84fdaa65605dd4770865c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed02a55bfc04f53bd6fef432cd65653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8bcfd1ca3841b298c784c10af3bc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6807661a774778a6e56cc778020adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47dd0a2bfce74b2f8d44517fa9576c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477e3861b16e42779f632b4ee17eb7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f9c1082f0246fa993442ad0b6ea7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6731690a74943f89e269c068483a68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c111142aa348018af39eec096fe3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94064c6221d34b7399c502313531644e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size 40:   0%|          | 0/66 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sort_names(name):\n",
    "    # Extract number\n",
    "    match = re.search(r\"(\\d+)\", name)\n",
    "    number = int(match.group(1)) if match else float(\"inf\")\n",
    "    return number\n",
    "\n",
    "\n",
    "sam_checkpoints = list(\n",
    "    (CHECKPOINT_DIRECTORY / \"decoder_only/resampled/2024-08-12 10:30:07/\").glob(\"*.pth\")\n",
    ")\n",
    "all_result_dicts = {}\n",
    "for chkpnt in sam_checkpoints:\n",
    "    sam_mevis.load_state_dict(\n",
    "        torch.load(\n",
    "            chkpnt,\n",
    "            map_location=torch.device(DEVICE),\n",
    "        ),\n",
    "        strict=True,\n",
    "    )\n",
    "    sam_mevis = sam_mevis.to(DEVICE)\n",
    "    result_dict_eval = eval_model(sam_mevis, dataset_test, loss_func=lossfunc)\n",
    "    result_dict_eval_merged = merge_metrics(result_dict_eval)\n",
    "    all_result_dicts[chkpnt.name] = result_dict_eval_merged\n",
    "    \n",
    "all_result_dicts = {\n",
    "    k: v for k, v in sorted(all_result_dicts.items(), key=lambda x: sort_names(x[0]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mevis_sam-epoch0.pth \n",
      " pixel_accuracy    0.976646\n",
      "iou               0.527404\n",
      "dice_score        0.538444\n",
      "DiceCELoss        1.527275 \n",
      "\n",
      "mevis_sam-epoch3.pth \n",
      " pixel_accuracy    0.982004\n",
      "iou               0.615592\n",
      "dice_score        0.615522\n",
      "DiceCELoss        1.520586 \n",
      "\n",
      "mevis_sam-epoch6.pth \n",
      " pixel_accuracy    0.981213\n",
      "iou               0.605275\n",
      "dice_score        0.621958\n",
      "DiceCELoss        1.521463 \n",
      "\n",
      "mevis_sam-epoch9.pth \n",
      " pixel_accuracy    0.982838\n",
      "iou               0.635008\n",
      "dice_score        0.648476\n",
      "DiceCELoss        1.519185 \n",
      "\n",
      "mevis_sam-epoch12.pth \n",
      " pixel_accuracy    0.982427\n",
      "iou               0.628734\n",
      "dice_score        0.651373\n",
      "DiceCELoss        1.519778 \n",
      "\n",
      "mevis_sam-epoch15.pth \n",
      " pixel_accuracy    0.982805\n",
      "iou                0.64226\n",
      "dice_score        0.665056\n",
      "DiceCELoss        1.518501 \n",
      "\n",
      "mevis_sam-epoch18.pth \n",
      " pixel_accuracy    0.982585\n",
      "iou               0.633171\n",
      "dice_score        0.660878\n",
      "DiceCELoss        1.519621 \n",
      "\n",
      "mevis_sam-epoch21.pth \n",
      " pixel_accuracy    0.983008\n",
      "iou               0.648179\n",
      "dice_score        0.677706\n",
      "DiceCELoss        1.517988 \n",
      "\n",
      "mevis_sam-epoch24.pth \n",
      " pixel_accuracy    0.983341\n",
      "iou                0.64485\n",
      "dice_score        0.672729\n",
      "DiceCELoss         1.51877 \n",
      "\n",
      "mevis_sam-epoch27.pth \n",
      " pixel_accuracy    0.983393\n",
      "iou               0.653847\n",
      "dice_score        0.680711\n",
      "DiceCELoss        1.517555 \n",
      "\n",
      "mevis_sam-epoch30.pth \n",
      " pixel_accuracy     0.98356\n",
      "iou               0.648511\n",
      "dice_score        0.678752\n",
      "DiceCELoss        1.518261 \n",
      "\n",
      "mevis_sam-epoch33.pth \n",
      " pixel_accuracy    0.984339\n",
      "iou                0.65798\n",
      "dice_score        0.681297\n",
      "DiceCELoss        1.517714 \n",
      "\n",
      "mevis_sam-epoch36.pth \n",
      " pixel_accuracy    0.984385\n",
      "iou               0.644653\n",
      "dice_score        0.669169\n",
      "DiceCELoss        1.519238 \n",
      "\n",
      "mevis_sam-epoch39.pth \n",
      " pixel_accuracy    0.984568\n",
      "iou               0.659997\n",
      "dice_score        0.681185\n",
      "DiceCELoss        1.517701 \n",
      "\n",
      "mevis_sam-epoch42.pth \n",
      " pixel_accuracy    0.984576\n",
      "iou               0.654523\n",
      "dice_score        0.675883\n",
      "DiceCELoss        1.518397 \n",
      "\n",
      "mevis_sam-epoch45.pth \n",
      " pixel_accuracy    0.985052\n",
      "iou               0.669418\n",
      "dice_score        0.690584\n",
      "DiceCELoss        1.517016 \n",
      "\n",
      "mevis_sam-epoch48.pth \n",
      " pixel_accuracy    0.982881\n",
      "iou               0.619414\n",
      "dice_score        0.636445\n",
      "DiceCELoss        1.520957 \n",
      "\n",
      "mevis_sam-epoch51.pth \n",
      " pixel_accuracy    0.983032\n",
      "iou                0.63904\n",
      "dice_score        0.657714\n",
      "DiceCELoss        1.518929 \n",
      "\n",
      "mevis_sam-epoch54.pth \n",
      " pixel_accuracy    0.982707\n",
      "iou               0.620478\n",
      "dice_score        0.638824\n",
      "DiceCELoss        1.520954 \n",
      "\n",
      "mevis_sam-epoch57.pth \n",
      " pixel_accuracy    0.983039\n",
      "iou                0.60942\n",
      "dice_score        0.615992\n",
      "DiceCELoss        1.522016 \n",
      "\n",
      "mevis_sam-epoch60.pth \n",
      " pixel_accuracy    0.983529\n",
      "iou               0.633876\n",
      "dice_score        0.656472\n",
      "DiceCELoss        1.519845 \n",
      "\n",
      "mevis_sam-epoch63.pth \n",
      " pixel_accuracy    0.984277\n",
      "iou               0.650786\n",
      "dice_score         0.66348\n",
      "DiceCELoss        1.518567 \n",
      "\n",
      "mevis_sam-epoch66.pth \n",
      " pixel_accuracy    0.983207\n",
      "iou               0.621185\n",
      "dice_score        0.641494\n",
      "DiceCELoss        1.521104 \n",
      "\n",
      "mevis_sam-epoch69.pth \n",
      " pixel_accuracy    0.984108\n",
      "iou               0.642034\n",
      "dice_score        0.657328\n",
      "DiceCELoss        1.519318 \n",
      "\n",
      "mevis_sam-epoch72.pth \n",
      " pixel_accuracy     0.98374\n",
      "iou               0.643458\n",
      "dice_score        0.668171\n",
      "DiceCELoss        1.518992 \n",
      "\n",
      "mevis_sam-epoch75.pth \n",
      " pixel_accuracy    0.984249\n",
      "iou                0.65086\n",
      "dice_score        0.677639\n",
      "DiceCELoss        1.518495 \n",
      "\n",
      "mevis_sam-epoch78.pth \n",
      " pixel_accuracy    0.983865\n",
      "iou               0.629958\n",
      "dice_score        0.648571\n",
      "DiceCELoss        1.520393 \n",
      "\n",
      "mevis_sam-epoch81.pth \n",
      " pixel_accuracy    0.981161\n",
      "iou               0.561995\n",
      "dice_score        0.590955\n",
      "DiceCELoss        1.525736 \n",
      "\n",
      "mevis_sam-epoch84.pth \n",
      " pixel_accuracy    0.981123\n",
      "iou               0.570652\n",
      "dice_score        0.609872\n",
      "DiceCELoss        1.524978 \n",
      "\n",
      "mevis_sam-epoch87.pth \n",
      " pixel_accuracy    0.982484\n",
      "iou               0.596686\n",
      "dice_score        0.626424\n",
      "DiceCELoss        1.522986 \n",
      "\n",
      "mevis_sam-epoch90-final.pth \n",
      " pixel_accuracy     0.97949\n",
      "iou               0.528803\n",
      "dice_score        0.558892\n",
      "DiceCELoss        1.528246 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chkpnt in sorted(sam_checkpoints, key=lambda x: sort_names(x.name)):\n",
    "    red_dict = all_result_dicts[chkpnt.name]\n",
    "    df = pd.DataFrame.from_dict(red_dict).T.drop(\"prediction_path\", axis=1)\n",
    "    print(chkpnt.name, \"\\n\", df.mean(axis=0).to_string(), \"\\n\")\n",
    "\n",
    "\n",
    "with open(\n",
    "    \"./eval_results/decoder_only_resampled_2024-08-12 10:30:07_all_models_test.json\",\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(\n",
    "        all_result_dicts,\n",
    "        f,\n",
    "        indent=4,\n",
    "        sort_keys=False,\n",
    "        separators=(\",\", \": \"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR.mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter(LOGDIR/\"learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(images, masks=None, atten_map=None, prompt=False):\n",
    "    bs = images.shape[0]\n",
    "    device = images.device\n",
    "    atten_map = torch.zeros((bs, 1, 64, 64), device=device) if atten_map == None else atten_map \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_embeddings = sam_mevis.image_encoder(images)\n",
    "        if prompt and masks != None:\n",
    "            images, point_coords, masks = generate_click_prompt(images, masks)\n",
    "            point_labels = torch.ones(images.size(0), device=device)\n",
    "            coords_torch = torch.as_tensor(\n",
    "                point_coords, dtype=torch.float, device=device\n",
    "            )\n",
    "            labels_torch = torch.as_tensor(point_labels, dtype=torch.int, device=device)\n",
    "            coords_torch, labels_torch = (\n",
    "                coords_torch[None, :, :],\n",
    "                labels_torch[None, :],\n",
    "            )\n",
    "            pt = (coords_torch, labels_torch)\n",
    "        else:\n",
    "            pt = None\n",
    "        sparse_embeddings, dense_embeddings = sam_mevis.prompt_encoder(\n",
    "            points=pt, boxes=None, masks=None\n",
    "        )\n",
    "\n",
    "        image_pe = torch.stack(\n",
    "            [sam_mevis.prompt_encoder.get_dense_pe() for i in range(bs)], dim=0\n",
    "        ).squeeze(1)\n",
    "        image_embeddings = sam_mevis.attention_fusion(image_embeddings, atten_map)\n",
    "        low_res_masks, iou_predictions = sam_mevis.mask_decoder(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=image_pe,\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=True,\n",
    "        )\n",
    "    return low_res_masks, iou_predictions\n",
    "\n",
    "sam_mevis.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Thesis_code/models/sam/modeling/tiny_vit_sam.py:347: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert L == H * W, \"input feature has wrong size\"\n",
      "/home/ubuntu/Thesis_code/models/sam/modeling/tiny_vit_sam.py:137: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  B = len(x)\n",
      "/home/ubuntu/Thesis_code/models/sam/modeling/mask_decoder.py:216: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if image_embeddings.shape[0] != tokens.shape[0]:\n",
      "/home/ubuntu/Thesis_code/models/sam/modeling/transformer.py:258: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  attn = attn / math.sqrt(c_per_head)\n"
     ]
    }
   ],
   "source": [
    "writer.add_graph(model=sam_mevis, input_to_model=dataset_test[2][\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask decoder 0 torch.Size([4, 256]) torch.Size([256, 64, 64])\n",
      "mask decoder 1 torch.Size([256, 4, 256]) torch.Size([256, 64, 64])\n",
      "Sizes of tensors must match except in dimension 1. Expected size 256 but got size 64 for tensor number 1 in the list.\n",
      "Error occurs, No graph saved\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 256 but got size 64 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: image_embeddings,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_prompt_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: sparse_embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultimask_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39mDEVICE),\n\u001b[1;32m     34\u001b[0m     }\n\u001b[1;32m     36\u001b[0m m_decoder_input \u001b[38;5;241m=\u001b[39m m_decoder_input_maker(mris_batched[\u001b[38;5;241m2\u001b[39m], sam_mevis)\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m---> 38\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msam_mevis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_to_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm_decoder_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/utils/tensorboard/writer.py:841\u001b[0m, in \u001b[0;36mSummaryWriter.add_graph\u001b[0;34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    838\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard.logging.add_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;66;03m# A valid PyTorch model should have a 'forward' method\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_graph(\n\u001b[0;32m--> 841\u001b[0m     \u001b[43mgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_to_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/utils/tensorboard/_pytorch_graph.py:337\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurs, No graph saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 337\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mprint\u001b[39m(graph)\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/utils/tensorboard/_pytorch_graph.py:331\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _set_model_to_eval(model):\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         trace \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m         graph \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mgraph\n\u001b[1;32m    333\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_jit_pass_inline(graph)\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/jit/_trace.py:1000\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    994\u001b[0m     check_if_torch_exportable,\n\u001b[1;32m    995\u001b[0m     log_torch_jit_trace_exportability,\n\u001b[1;32m    996\u001b[0m     log_torchscript_usage,\n\u001b[1;32m    997\u001b[0m )\n\u001b[1;32m    999\u001b[0m log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1000\u001b[0m traced_func \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_compilation_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_if_torch_exportable():\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_export\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconverter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TS2EPConverter\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/jit/_trace.py:695\u001b[0m, in \u001b[0;36m_trace_impl\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    712\u001b[0m ):\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/jit/_trace.py:1275\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1274\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1275\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/nn/modules/module.py:1543\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1543\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Thesis_code/models/sam/modeling/mask_decoder.py:184\u001b[0m, in \u001b[0;36mMaskDecoder.forward\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    163\u001b[0m     image_embeddings: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m     multimask_output: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    168\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    Predict masks given image and prompt embeddings.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m      torch.Tensor: batched predictions of mask quality\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     masks, iou_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_masks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_pe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# Select the correct mask or masks for output\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multimask_output:\n",
      "File \u001b[0;32m~/Thesis_code/models/sam/modeling/mask_decoder.py:216\u001b[0m, in \u001b[0;36mMaskDecoder.predict_masks\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)\u001b[0m\n\u001b[1;32m    214\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_tokens\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(sparse_prompt_embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask decoder 1\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_tokens\u001b[38;5;241m.\u001b[39mshape, sparse_prompt_embeddings\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 216\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Expand per-image data in batch direction to be per-mask\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 256 but got size 64 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# try to make training loop\n",
    "def m_decoder_input_maker(batch, sam):\n",
    "    bs = batch[\"images\"].shape[0]\n",
    "    image_embeddings = sam.image_encoder(batch[\"images\"])\n",
    "    sparse_embeddings, dense_embeddings = sam.prompt_encoder(\n",
    "        points=None, boxes=None, masks=None\n",
    "    )\n",
    "    sparse_embeddings = sparse_embeddings.squeeze(0)\n",
    "    dense_embeddings = dense_embeddings.squeeze(0)\n",
    "\n",
    "    image_pe = torch.stack(\n",
    "        [sam.prompt_encoder.get_dense_pe() for i in range(bs)], dim=0\n",
    "    ).squeeze(1)\n",
    "    for i in range(image_embeddings.shape[0]):\n",
    "        image_embeddings[i] = sam.attention_fusion(\n",
    "            image_embeddings[i : i + 1], batch[\"atten_map\"][i : i + 1]\n",
    "        )\n",
    "    # low_res_masks, iou_predictions = sam.mask_decoder(\n",
    "    #     image_embeddings=image_embeddings,\n",
    "    #     image_pe=image_pe,\n",
    "    #     sparse_prompt_embeddings=sparse_embeddings,\n",
    "    #     dense_prompt_embeddings=dense_embeddings,\n",
    "    #     multimask_output=True,\n",
    "    # )\n",
    "\n",
    "    # low_res_masks_bool = low_res_masks > sam_fine_tune.mask_threshold\n",
    "    \n",
    "    return {\n",
    "        \"image_embeddings\": image_embeddings,\n",
    "        \"sparse_prompt_embeddings\": sparse_embeddings,\n",
    "        \"dense_prompt_embeddings\": dense_embeddings,\n",
    "        \"image_pe\": image_pe,\n",
    "        \"multimask_output\": torch.tensor(True, dtype=torch.bool, device=DEVICE),\n",
    "    }\n",
    "\n",
    "m_decoder_input = m_decoder_input_maker(mris_batched[2], sam_mevis).values()\n",
    "\n",
    "writer.add_graph(\n",
    "    model=sam_mevis.mask_decoder,\n",
    "    input_to_model=m_decoder_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      2\u001b[0m     inp \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m         {\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m         }\n\u001b[1;32m      8\u001b[0m     ]\n\u001b[1;32m      9\u001b[0m     out \u001b[38;5;241m=\u001b[39m sam_mevis(inp)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not iterable"
     ]
    }
   ],
   "source": [
    "sam_mevis.train()\n",
    "for param in sam_mevis.parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in sam_mevis.image_encoder.named_parameters():\n",
    "    if name in image_encoder_layers:\n",
    "        param.requires_grad = True\n",
    "for name, param in sam_mevis.mask_decoder.named_parameters():\n",
    "    if name in mask_decoder_layers:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.data import DataLoader\\nfrom torch.utils.tensorboard import SummaryWriter\\n\\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, writer):\\n    for epoch in range(num_epochs):\\n        model.train()\\n        running_loss = 0.0\\n        for i, (inputs, labels) in enumerate(train_loader):\\n            optimizer.zero_grad()\\n            outputs = model(inputs)\\n            loss = criterion(outputs, labels)\\n            loss.backward()\\n            optimizer.step()\\n            running_loss += loss.item()\\n\\n            # Log training loss every 100 batches\\n            if i % 100 == 99:\\n                writer.add_scalar('Training Loss', running_loss / 100, epoch * len(train_loader) + i)\\n                running_loss = 0.0\\n\\n        validate_model(model, val_loader, criterion, writer, epoch)\\n\\ndef validate_model(model, val_loader, criterion, writer, epoch):\\n    model.eval()\\n    val_loss = 0.0\\n    correct = 0\\n    total = 0\\n    with torch.no_grad():\\n        for inputs, labels in val_loader:\\n            outputs = model(inputs)\\n            loss = criterion(outputs, labels)\\n            val_loss += loss.item()\\n            _, predicted = torch.max(outputs.data, 1)\\n            total += labels.size(0)\\n            correct += (predicted == labels).sum().item()\\n\\n    avg_val_loss = val_loss / len(val_loader)\\n    accuracy = 100 * correct / total\\n    writer.add_scalar('Validation Loss', avg_val_loss, epoch)\\n    writer.add_scalar('Validation Accuracy', accuracy, epoch)\\n\\n    print(f\\\"Validation Loss: {avg_val_loss}, Accuracy: {accuracy}%\\\")\\n\\n# Example usage\\nif __name__ == \\\"__main__\\\":\\n    model = ...  # Your model here\\n    train_dataset = ...  # Your training dataset here\\n    val_dataset = ...  # Your validation dataset here\\n\\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\\n\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\\n    num_epochs = 10\\n\\n    # Initialize TensorBoard writer\\n    writer = SummaryWriter('runs/experiment_1')\\n\\n    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, writer)\\n\\n    # Close the writer\\n    writer.close()\\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
