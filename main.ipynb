{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mlflow server --host 0.0.0.0 --port 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-3b133eb8-ed94-51f8-937f-cbc3e3f3ff2a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MIG-3b133eb8-ed94-51f8-937f-cbc3e3f3ff2a'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-3b133eb8-ed94-51f8-937f-cbc3e3f3ff2a\n",
    "#MIG-4f53211e-ceab-5aa7-b0ba-6ef87e62c8ed\n",
    "%env CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Thesis_code/models/sam/modeling/tiny_vit_sam.py:677: UserWarning: Overwriting tiny_vit_5m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/ubuntu/Thesis_code/models/sam/modeling/tiny_vit_sam.py:677: UserWarning: Overwriting tiny_vit_11m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/ubuntu/Thesis_code/models/sam/modeling/tiny_vit_sam.py:677: UserWarning: Overwriting tiny_vit_21m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/ubuntu/Thesis_code/models/sam/modeling/tiny_vit_sam.py:677: UserWarning: Overwriting tiny_vit_21m_384 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/ubuntu/Thesis_code/models/sam/modeling/tiny_vit_sam.py:677: UserWarning: Overwriting tiny_vit_21m_512 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "from monai.losses import DiceCELoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "\n",
    "import cfg\n",
    "from dataset_mevis_v2 import MRI_dataset_batched\n",
    "from models.sam import build_sam_mevis\n",
    "from retrain_v2 import lr_scheduler, train_mevis, validate_mevis, write_hists\n",
    "\n",
    "# TODO:\n",
    "# run signature with bone-sam address and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mevis_args_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"Mevis SAM fine-tuning parameters.\")\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint_path\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Checkpoint to continue the training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prompt_probability\",\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help=\"probability of generating prompts for each batch\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_schedule\",\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help=\"Use earning rate scheduler during training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_train_start\",\n",
    "        type=float,\n",
    "        default=5e-4,\n",
    "        help=\"Learning rate starting value during training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_train_end\",\n",
    "        type=float,\n",
    "        default=5e-5,\n",
    "        help=\"Learning rate ending value during training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_warmup\", type=float, default=1e-5, help=\"Learning rate on warmup.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\", type=int, default=100, help=\"Number of training epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs_warmup\", type=int, default=20, help=\"Number of warmup epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=120, help=\"Number of slices in a batch\"\n",
    "    )\n",
    "    mevis_args = parser.parse_args(\"\")\n",
    "\n",
    "    return mevis_args\n",
    "\n",
    "\n",
    "mevis_args = mevis_args_parser()\n",
    "args = cfg.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_server = \"http://0.0.0.0:5000\"\n",
    "TRAIN_DATA_FILE = \"data_files/Train_data_files.json\"\n",
    "VALID_DATA_FILE = \"data_files/Validation_data_files.json\"\n",
    "DEVICE = torch.device(\"cuda:\" + str(args.gpu_device))\n",
    "CHECKPOINT_DIRECTORY = Path(\"/data/sab_data/checkpoints\")\n",
    "LOGDIR = Path(\"/data/sab_data/model_logs/mlflow\")\n",
    "# train_folder = \"decoder_only/30p_prompt_v2\"\n",
    "BONE_CHECKPOINT = CHECKPOINT_DIRECTORY / \"bone_sam.pth\"\n",
    "VALID_EVERY = 2\n",
    "SAVE_EVERY = 3\n",
    "OPEN_LAYERS = (\n",
    "    \"mask_decoder.transformer.layers.0.MLP_Adapter,\"\n",
    "    \"mask_decoder.transformer.layers.0.Adapter,\"\n",
    "    \"mask_decoder.transformer.layers.1.MLP_Adapter,\"\n",
    "    \"mask_decoder.transformer.layers.1.Adapter,\"\n",
    ")\n",
    "merged_args = argparse.Namespace(**vars(args), **vars(mevis_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"sample_name\"\n",
    "experiment_description = (\n",
    "    \"test test This is fine-tuning of SAB model for vertebrae. \"\n",
    "    \"The improvements from first version are applied.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"new_proj\",\n",
    "    \"version\": \"9.0\",\n",
    "    \"project_time\": \"October-2024\",\n",
    "    \"opened_layers\": OPEN_LAYERS,\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "# Create the Experiment, providing a unique name\n",
    "mlflow.set_tracking_uri(uri=tracking_server)\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if experiment is None:\n",
    "    mlflow.create_experiment(\n",
    "        name=experiment_name,\n",
    "        tags=experiment_tags,\n",
    "    )\n",
    "    experiment = mlflow.set_experiment(experiment_name)\n",
    "log_base = LOGDIR / experiment.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = MRI_dataset_batched(\n",
    "        merged_args,\n",
    "        data_file=VALID_DATA_FILE,\n",
    "        batch_size=mevis_args.batch_size,\n",
    "        phase=\"test\",\n",
    "        operation_mode=\"queue\",\n",
    "        mask_out_size=merged_args.out_size,\n",
    "        attention_size=64,\n",
    "        crop=False,\n",
    "        crop_size=1024,\n",
    "        cls=1,\n",
    "        if_prompt=True,\n",
    "        prompt_type=\"points\",\n",
    "        if_attention_map=True,\n",
    "        device=DEVICE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset_validation)):\n",
    "    batch = dataset_validation[i]\n",
    "    if batch.get(\"points\", None) is not None:\n",
    "        points = batch[\"points\"]\n",
    "        print(batch[\"points\"].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['slices', 'original_size', 'image_name', 'images', 'masks', 'atten_maps', 'cat_indexes', 'points', 'point_labels'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Thesis_code/models/sam/build_sam.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n",
      "/tmp/ipykernel_125483/51230101.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    }
   ],
   "source": [
    "from models.sam import sam_model_registry\n",
    "from models.sam.modeling.prompt_encoder import Attention_Fusion\n",
    "\n",
    "def forward(\n",
    "        self,\n",
    "        batched_input: dict,\n",
    "        multimask_output: bool = True,\n",
    "        if_attention=False,\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Predicts masks end-to-end from provided images and prompts.\n",
    "        If prompts are not known in advance, using SamPredictor is\n",
    "        recommended over calling the model directly.\n",
    "\n",
    "        Arguments:\n",
    "            batched_input (dict): A dictionary with the following keys. A prompt key can be\n",
    "            excluded if it is not present.\n",
    "                'images': The image as a torch tensor in 3xHxW format,\n",
    "                already transformed for input to the model.\n",
    "                'points': (torch.Tensor) Batched point prompts for\n",
    "                this image, with shape BxNx2. Already transformed to the\n",
    "                input frame of the model.\n",
    "                'point_labels': (torch.Tensor) Batched labels for point prompts,\n",
    "                with shape BxN.\n",
    "                'atten_maps\": (optional)(torch.Tensor) Batched attention maps, with\n",
    "                shape Bx64x64.\n",
    "                'boxes': (torch.Tensor) Batched box inputs, with shape Bx4.\n",
    "                Already transformed to the input frame of the model.\n",
    "                'mask_inputs': (torch.Tensor) Batched mask inputs to the model,\n",
    "                in the form Bx1xHxW.\n",
    "            multimask_output (bool): Whether the model should predict multiple\n",
    "            disambiguating masks, or return a single mask.\n",
    "\n",
    "        Returns:\n",
    "            (list(dict)): A list over input images, where each element is\n",
    "            as dictionary with the following keys.\n",
    "                'masks': (torch.Tensor) Batched binary mask predictions,\n",
    "                with shape BxCxHxW, where B is the number of input prompts,\n",
    "                C is determined by multimask_output, and (H, W) is the\n",
    "                original size of the image.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            image_embeddings = self.image_encoder(batched_input[\"images\"])\n",
    "            if if_attention:\n",
    "                image_embeddings = self.attention_fusion(\n",
    "                    image_embeddings, batched_input[\"atten_maps\"].unsqueeze(1)\n",
    "                )\n",
    "\n",
    "            if \"points\" in list(batched_input.keys()):\n",
    "                points = (batched_input[\"points\"], batched_input[\"point_labels\"])\n",
    "            else:\n",
    "                points = None\n",
    "\n",
    "            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "                points=points,\n",
    "                boxes=batched_input.get(\"boxes\", None),\n",
    "                masks=batched_input.get(\"mask_inputs\", None),\n",
    "            )\n",
    "            pe = torch.stack(\n",
    "                [self.prompt_encoder.get_dense_pe() for _ in range(image_embeddings.shape[0])],\n",
    "                dim=0,\n",
    "            ).squeeze(1)\n",
    "            ###\n",
    "            print(\"sam_mevis\", pe.shape)\n",
    "            ###\n",
    "            \n",
    "        low_res_masks, iou_predictions = self.mask_decoder(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=pe,\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=multimask_output,\n",
    "        )\n",
    "\n",
    "        return low_res_masks\n",
    "\n",
    "sam_mevis = sam_model_registry[\"vit_t\"](\n",
    "    args, checkpoint=CHECKPOINT_DIRECTORY / \"mobile_sam.pt\", num_classes=2\n",
    ")\n",
    "sam_mevis.attention_fusion = Attention_Fusion()\n",
    "sam_mevis.load_state_dict(\n",
    "    torch.load(\n",
    "        CHECKPOINT_DIRECTORY / BONE_CHECKPOINT,\n",
    "        map_location=torch.device(DEVICE),\n",
    "    ),\n",
    "    strict=True,\n",
    ")\n",
    "# sam_mevis.forward = forward\n",
    "sam_mevis = sam_mevis.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_mevis = build_sam_mevis(\n",
    "        merged_args,\n",
    "        mevis_checkpoint=CHECKPOINT_DIRECTORY / BONE_CHECKPOINT,\n",
    "        num_classes=2,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "open_layers = [x for x in OPEN_LAYERS.split(\",\") if x != \"\"]\n",
    "for param in sam_mevis.parameters():\n",
    "    param.requires_grad = False\n",
    "for name, mod in sam_mevis.named_parameters():\n",
    "    for l in open_layers:\n",
    "        if l in name:\n",
    "            mod.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 256, 64, 64]) torch.Size([120, 1, 256, 64, 64])\n",
      "mask_decoder torch.Size([14400, 256, 64, 64]) torch.Size([120, 256, 64, 64]) torch.Size([120, 256, 64, 64]) torch.Size([120, 256, 64, 64]) torch.Size([120, 6, 256])\n",
      "transformer torch.Size([120, 6, 256]) torch.Size([120, 4096, 256]) torch.Size([14400, 4096, 256]) torch.Size([120, 6, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (120) must match the size of tensor b (14400) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     17\u001b[0m         pe\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m     18\u001b[0m         torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         )\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     imge \u001b[38;5;241m=\u001b[39m sam_mevis\u001b[38;5;241m.\u001b[39mattention_fusion(imge, attens\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 24\u001b[0m pred, _ \u001b[38;5;241m=\u001b[39m \u001b[43msam_mevis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimge\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis_code/models/sam/modeling/mask_decoder.py:184\u001b[0m, in \u001b[0;36mMaskDecoder.forward\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    163\u001b[0m     image_embeddings: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m     multimask_output: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    168\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    Predict masks given image and prompt embeddings.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m      torch.Tensor: batched predictions of mask quality\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     masks, iou_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_masks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_pe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# Select the correct mask or masks for output\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multimask_output:\n",
      "File \u001b[0;32m~/Thesis_code/models/sam/modeling/mask_decoder.py:228\u001b[0m, in \u001b[0;36mMaskDecoder.predict_masks\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)\u001b[0m\n\u001b[1;32m    225\u001b[0m b, c, h, w \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Run the transformer\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m hs, src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m iou_token_out \u001b[38;5;241m=\u001b[39m hs[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m    230\u001b[0m mask_tokens_out \u001b[38;5;241m=\u001b[39m hs[:, \u001b[38;5;241m1\u001b[39m : (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_mask_tokens), :]\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis_code/models/sam/modeling/transformer.py:102\u001b[0m, in \u001b[0;36mTwoWayTransformer.forward\u001b[0;34m(self, image_embedding, image_pe, point_embedding)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m####\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Apply transformer blocks and final layernorm\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 102\u001b[0m     queries, keys \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoint_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_pe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_pe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Apply the final attention layer from the points to the image\u001b[39;00m\n\u001b[1;32m    110\u001b[0m q \u001b[38;5;241m=\u001b[39m queries \u001b[38;5;241m+\u001b[39m point_embedding\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sab_env2/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis_code/models/sam/modeling/transformer.py:183\u001b[0m, in \u001b[0;36mTwoWayAttentionBlock.forward\u001b[0;34m(self, queries, keys, query_pe, key_pe)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Cross attention block, tokens attending to image embedding\u001b[39;00m\n\u001b[1;32m    182\u001b[0m q \u001b[38;5;241m=\u001b[39m queries \u001b[38;5;241m+\u001b[39m query_pe\n\u001b[0;32m--> 183\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mkeys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey_pe\u001b[49m\n\u001b[1;32m    184\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_token_to_image(q\u001b[38;5;241m=\u001b[39mq, k\u001b[38;5;241m=\u001b[39mk, v\u001b[38;5;241m=\u001b[39mkeys)\n\u001b[1;32m    185\u001b[0m queries \u001b[38;5;241m=\u001b[39m queries \u001b[38;5;241m+\u001b[39m attn_out\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (120) must match the size of tensor b (14400) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "imgs = batch[\"images\"]\n",
    "masks = batch[\"masks\"]\n",
    "attens = batch[\"atten_maps\"]\n",
    "pt = (batch[\"points\"], batch[\"point_labels\"])\n",
    "with torch.no_grad():\n",
    "    imge = sam_mevis.image_encoder(imgs)\n",
    "    se, de = sam_mevis.prompt_encoder(\n",
    "        points=pt,\n",
    "        boxes=None,\n",
    "        masks=None,\n",
    "    )\n",
    "    pe = torch.stack(\n",
    "        [sam_mevis.prompt_encoder.get_dense_pe() for _ in range(imge.shape[0])],\n",
    "        dim=0,\n",
    "    ).squeeze(1)\n",
    "    print(\n",
    "        pe.shape,\n",
    "        torch.stack(\n",
    "            [sam_mevis.prompt_encoder.get_dense_pe() for _ in range(imge.shape[0])],\n",
    "            dim=0,\n",
    "        ).shape,\n",
    "    )\n",
    "    imge = sam_mevis.attention_fusion(imge, attens.unsqueeze(1))\n",
    "pred, _ = sam_mevis.mask_decoder(\n",
    "    image_embeddings=imge,\n",
    "    image_pe=pe,\n",
    "    sparse_prompt_embeddings=se,\n",
    "    dense_prompt_embeddings=de,\n",
    "    multimask_output=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sam_mevis torch.Size([1, 256, 64, 64])\n",
      "mask_decoder torch.Size([120, 256, 64, 64]) torch.Size([120, 256, 64, 64]) torch.Size([1, 256, 64, 64]) torch.Size([120, 256, 64, 64]) torch.Size([120, 6, 256])\n",
      "transformer torch.Size([120, 6, 256]) torch.Size([120, 4096, 256]) torch.Size([120, 4096, 256]) torch.Size([120, 6, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[  4.6366,   4.6843,   4.7206,  ...,   4.9741,   4.7001,   5.1016],\n",
       "          [  4.5377,   5.8957,   4.8945,  ...,   5.6354,   5.0819,   5.1050],\n",
       "          [  4.8227,   5.3301,   5.1379,  ...,   5.4952,   4.8372,   4.7834],\n",
       "          ...,\n",
       "          [  4.2775,   5.3811,   4.7832,  ...,   4.9188,   4.5895,   4.7840],\n",
       "          [  5.3820,   5.1543,   5.1019,  ...,   5.0971,   4.7584,   5.2836],\n",
       "          [  5.1297,   5.7867,   4.9409,  ...,   5.3520,   4.9922,   5.9886]],\n",
       "\n",
       "         [[ -9.5473,  -9.4737,  -9.1879,  ...,  -7.4058,  -7.3565,  -7.9861],\n",
       "          [ -9.5368,  -9.8512,  -9.3096,  ...,  -7.0785,  -7.7052,  -7.6426],\n",
       "          [-10.1158, -10.7048, -10.8573,  ...,  -7.9592,  -8.0751,  -7.7347],\n",
       "          ...,\n",
       "          [-10.1512, -10.3841, -11.2979,  ...,  -7.7637,  -8.3725,  -8.4147],\n",
       "          [-12.4639, -12.9783, -12.8968,  ...,  -9.2074, -10.5699, -10.4548],\n",
       "          [-13.0393, -13.9144, -12.6940,  ..., -10.2157, -11.1949, -12.8792]]],\n",
       "\n",
       "\n",
       "        [[[  4.5127,   4.5239,   4.6171,  ...,   4.7090,   4.4704,   4.8825],\n",
       "          [  4.4173,   5.6335,   4.7666,  ...,   5.2904,   4.8263,   4.8341],\n",
       "          [  4.6250,   5.1161,   5.0468,  ...,   5.1191,   4.5910,   4.5798],\n",
       "          ...,\n",
       "          [  4.2886,   5.3226,   4.9255,  ...,   4.7713,   4.5558,   4.7073],\n",
       "          [  5.3064,   5.0619,   5.1992,  ...,   4.9739,   4.7350,   5.2271],\n",
       "          [  5.0687,   5.6364,   4.9867,  ...,   5.1454,   4.9155,   5.8359]],\n",
       "\n",
       "         [[ -9.6405,  -9.5840,  -9.0074,  ...,  -7.3106,  -7.1064,  -7.7605],\n",
       "          [ -9.4898,  -9.7712,  -9.1024,  ...,  -6.8588,  -7.3655,  -7.4655],\n",
       "          [-10.0371, -10.7495, -10.5936,  ...,  -7.7247,  -7.8198,  -7.4666],\n",
       "          ...,\n",
       "          [-10.9642, -11.1077, -11.9444,  ...,  -8.1480,  -8.6059,  -8.6506],\n",
       "          [-13.1434, -13.8356, -13.4174,  ...,  -9.6083, -10.7703, -10.6451],\n",
       "          [-13.4844, -14.6558, -13.0157,  ..., -10.5571, -11.2292, -13.1307]]],\n",
       "\n",
       "\n",
       "        [[[  5.0655,   4.9831,   5.3074,  ...,   5.2582,   4.9685,   5.5169],\n",
       "          [  5.0286,   6.5069,   5.5890,  ...,   6.2851,   5.6138,   5.6399],\n",
       "          [  5.4732,   5.9236,   5.3292,  ...,   6.0452,   5.0784,   5.3669],\n",
       "          ...,\n",
       "          [  4.4402,   5.9410,   4.9392,  ...,   5.5088,   5.1033,   5.0711],\n",
       "          [  5.9293,   5.3689,   4.9508,  ...,   5.4370,   4.8109,   5.6340],\n",
       "          [  4.9057,   5.6835,   4.6598,  ...,   5.6882,   4.9047,   6.7300]],\n",
       "\n",
       "         [[ -8.5836,  -8.2485,  -7.8133,  ...,  -5.1200,  -5.0726,  -5.8349],\n",
       "          [ -8.6563,  -8.8107,  -8.3521,  ...,  -4.7933,  -5.7247,  -5.5013],\n",
       "          [ -8.9165,  -8.8587,  -9.2717,  ...,  -5.6605,  -6.0161,  -5.8895],\n",
       "          ...,\n",
       "          [ -9.3754,  -9.4549, -10.7799,  ...,  -5.6146,  -6.0273,  -5.9254],\n",
       "          [-11.6198, -11.3285, -11.9834,  ...,  -6.5154,  -8.0588,  -7.8934],\n",
       "          [-12.1685, -12.7240, -12.1156,  ...,  -8.0046,  -9.5068, -10.4653]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[  3.8197,   3.8353,   3.8067,  ...,   4.2047,   4.2002,   4.4976],\n",
       "          [  3.7463,   4.4799,   3.9619,  ...,   4.7069,   4.5243,   4.4447],\n",
       "          [  3.6542,   3.9229,   4.1894,  ...,   4.8001,   4.5587,   4.2807],\n",
       "          ...,\n",
       "          [  4.3605,   5.0980,   4.8200,  ...,   4.3936,   4.1386,   4.3547],\n",
       "          [  5.3347,   4.9781,   5.4037,  ...,   4.5661,   4.6598,   4.7829],\n",
       "          [  5.1810,   5.6717,   5.0801,  ...,   4.9191,   4.6891,   5.6266]],\n",
       "\n",
       "         [[ -6.9914,  -6.4766,  -6.9224,  ...,  -5.8294,  -6.3165,  -6.4570],\n",
       "          [ -6.8956,  -6.9016,  -6.7443,  ...,  -6.1903,  -6.4672,  -6.5407],\n",
       "          [ -7.5027,  -7.4271,  -7.8865,  ...,  -7.2090,  -7.1872,  -7.0975],\n",
       "          ...,\n",
       "          [ -9.1073,  -9.3040,  -9.7453,  ...,  -6.7077,  -6.9009,  -7.2440],\n",
       "          [-11.6390, -12.1056, -12.0620,  ...,  -8.1596,  -9.3391,  -9.2023],\n",
       "          [-12.2138, -12.6619, -11.3726,  ...,  -9.1163,  -9.4959, -10.8642]]],\n",
       "\n",
       "\n",
       "        [[[  3.5352,   3.5442,   3.2652,  ...,   4.6188,   4.5151,   4.8169],\n",
       "          [  3.6466,   4.2523,   3.4654,  ...,   4.9190,   4.7227,   4.7534],\n",
       "          [  3.6839,   3.6679,   3.7990,  ...,   4.8502,   4.7789,   4.3035],\n",
       "          ...,\n",
       "          [  4.3749,   5.0147,   4.3320,  ...,   3.8927,   3.2862,   3.8640],\n",
       "          [  5.3576,   4.7980,   5.2116,  ...,   4.0220,   4.1326,   4.1910],\n",
       "          [  5.1510,   5.6672,   5.0429,  ...,   4.5195,   4.2961,   5.4414]],\n",
       "\n",
       "         [[ -7.2937,  -6.7636,  -6.7278,  ...,  -7.4305,  -7.8563,  -8.2811],\n",
       "          [ -7.7088,  -7.2297,  -6.4063,  ...,  -7.5836,  -7.6188,  -7.9284],\n",
       "          [ -8.5232,  -8.1323,  -8.0263,  ...,  -9.0385,  -8.4746,  -8.3458],\n",
       "          ...,\n",
       "          [-10.1128,  -9.7398,  -9.5951,  ...,  -6.0040,  -5.8324,  -6.7743],\n",
       "          [-12.8284, -13.2318, -12.9024,  ...,  -8.0001,  -9.0893,  -8.3477],\n",
       "          [-13.3753, -13.6440, -12.5328,  ...,  -8.8971,  -9.7444, -10.3373]]],\n",
       "\n",
       "\n",
       "        [[[  3.6925,   3.8375,   3.6677,  ...,   4.2581,   4.0104,   4.3697],\n",
       "          [  3.6508,   4.4782,   3.8592,  ...,   4.7645,   4.4276,   4.4571],\n",
       "          [  3.6502,   3.7944,   4.0917,  ...,   4.8160,   4.4153,   4.1053],\n",
       "          ...,\n",
       "          [  4.2089,   5.0230,   4.5564,  ...,   4.2370,   3.8415,   4.2207],\n",
       "          [  5.2546,   4.7267,   5.0587,  ...,   4.3806,   4.3165,   4.5522],\n",
       "          [  4.8726,   5.4866,   4.7739,  ...,   4.6941,   4.3884,   5.4736]],\n",
       "\n",
       "         [[ -7.0273,  -6.7225,  -6.8219,  ...,  -5.9921,  -6.1459,  -6.4533],\n",
       "          [ -7.2568,  -7.0359,  -6.8323,  ...,  -6.2679,  -6.4554,  -6.5779],\n",
       "          [ -8.0716,  -7.8243,  -8.2529,  ...,  -7.6452,  -7.2278,  -7.3288],\n",
       "          ...,\n",
       "          [ -9.7255,  -9.8320, -10.1671,  ...,  -6.6639,  -6.8833,  -7.4365],\n",
       "          [-12.5481, -13.0185, -12.7840,  ...,  -8.6022,  -9.7512,  -9.8379],\n",
       "          [-12.8208, -13.3678, -11.9390,  ...,  -9.3595,  -9.8538, -11.4139]]]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_mevis.forward(batched_input=batch, multimask_output=True, if_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_procedure(\n",
    "    run,\n",
    "    epoch,\n",
    "    log_base,\n",
    "    mevis_args,\n",
    "    merged_args,\n",
    "):\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    # Tensorboard logger\n",
    "    log_save = log_base / run_id\n",
    "    log_save.mkdir(parents=True, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=log_save)\n",
    "\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(vars(mevis_args))\n",
    "\n",
    "    # data\n",
    "    dataset_validation = MRI_dataset_batched(\n",
    "        merged_args,\n",
    "        data_file=VALID_DATA_FILE,\n",
    "        batch_size=mevis_args.batch_size,\n",
    "        phase=\"test\",\n",
    "        operation_mode=\"queue\",\n",
    "        mask_out_size=merged_args.out_size,\n",
    "        attention_size=64,\n",
    "        crop=False,\n",
    "        crop_size=1024,\n",
    "        cls=1,\n",
    "        if_prompt=True,\n",
    "        prompt_type=\"points\",\n",
    "        if_attention_map=True,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    dataset_train = MRI_dataset_batched(\n",
    "        merged_args,\n",
    "        data_file=TRAIN_DATA_FILE,\n",
    "        batch_size=mevis_args.batch_size,\n",
    "        phase=\"train\",\n",
    "        operation_mode=\"queue\",\n",
    "        mask_out_size=merged_args.out_size,\n",
    "        attention_size=64,\n",
    "        crop=False,\n",
    "        crop_size=1024,\n",
    "        cls=1,\n",
    "        if_prompt=True,\n",
    "        prompt_type=\"points\",\n",
    "        if_attention_map=True,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    # model\n",
    "    sam_mevis = build_sam_mevis(\n",
    "        merged_args,\n",
    "        mevis_checkpoint=CHECKPOINT_DIRECTORY / BONE_CHECKPOINT,\n",
    "        num_classes=2,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    open_layers = [x for x in OPEN_LAYERS.split(\",\") if x != \"\"]\n",
    "    for param in sam_mevis.parameters():\n",
    "        param.requires_grad = False\n",
    "    for name, mod in sam_mevis.named_parameters():\n",
    "        for l in open_layers:\n",
    "            if l in name:\n",
    "                mod.requires_grad = True\n",
    "\n",
    "    total_trainable_params = sum(\n",
    "        p.numel() for p in sam_mevis.parameters() if p.requires_grad\n",
    "    )\n",
    "    mlflow.log_param(\"total_trainable_params\", total_trainable_params)\n",
    "    print(\"Number of trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "    lossfunc = DiceCELoss(sigmoid=True, squared_pred=True, reduction=\"mean\")\n",
    "    optimizer = torch.optim.AdamW(sam_mevis.parameters(), lr=mevis_args.lr_train_start)\n",
    "\n",
    "    while epoch < (merged_args.lr_warmup + merged_args.epochs):\n",
    "        optimizer = lr_scheduler(\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            n_warmup_epochs=merged_args.epochs_warmup,\n",
    "            n_epochs=merged_args.epochs,\n",
    "            base_lr=merged_args.lr_train_start,\n",
    "            warmup_lr_start=merged_args.lr_warmup,\n",
    "            end_lr=merged_args.lr_train_end,\n",
    "        )\n",
    "        dataset_train.on_epoch_end()\n",
    "        loss = train_mevis(\n",
    "            net=sam_mevis,\n",
    "            optimizer=optimizer,\n",
    "            loss_func=lossfunc,\n",
    "            dataset=dataset_train,\n",
    "            epoch=epoch,\n",
    "            writer=writer,\n",
    "        )\n",
    "        mlflow.log_metric(key=\"DiceCE\", value=loss, step=epoch)\n",
    "        writer.add_scalars(\"DiceCEloss\", {\"training\": loss}, epoch)\n",
    "        write_hists(sam_mevis, writer=writer, epoch=epoch)\n",
    "        if epoch % VALID_EVERY == 0:\n",
    "            val_loss, val_result = validate_mevis(\n",
    "                net=sam_mevis,\n",
    "                loss_func=lossfunc,\n",
    "                dataset=dataset_validation,\n",
    "                epoch=epoch,\n",
    "                writer=writer,\n",
    "            )\n",
    "            writer.add_scalars(\"DiceCEloss\", {\"validation\": val_loss}, epoch)\n",
    "            for key, value in val_result.items():\n",
    "                for key_in, value_in in value.items():\n",
    "                    writer.add_scalar(\n",
    "                        f\"Validation Metrics/{key_in}/thr{key}\", value_in, epoch\n",
    "                    )\n",
    "        if epoch % SAVE_EVERY == 0:\n",
    "            mlflow.pytorch.log_model(\n",
    "                sam_mevis, artifact_path=f\"mevis_sam-epoch{epoch}-{run_id}\"\n",
    "            )\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    mlflow.pytorch.log_model(\n",
    "        sam_mevis, artifact_path=f\"mevis_sam-epoch{epoch}-{run_id}\"\n",
    "    )\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in the model: 132352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 85/85 [07:03<00:00,  4.98s/batch, loss (batch)=0.421]\n",
      "Validation round: 100%|██████████| 10/10 [00:25<00:00,  2.57s/batch, loss (batch)=0.446]\n",
      "2024/10/23 09:48:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Epoch 1:  49%|████▉     | 42/85 [03:21<03:26,  4.80s/batch, loss (batch)=0.525]\n",
      "2024/10/23 09:51:24 INFO mlflow.tracking._tracking_service.client: 🏃 View run flawless-shark-609 at: http://0.0.0.0:5000/#/experiments/662304808085456660/runs/b1efbd001bfa4687a23386c0d2dbc57a.\n",
      "2024/10/23 09:51:24 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://0.0.0.0:5000/#/experiments/662304808085456660.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run() \u001b[38;5;28;01mas\u001b[39;00m parent:\n\u001b[0;32m---> 26\u001b[0m         \u001b[43mtrain_procedure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmevis_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmevis_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmerged_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerged_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 87\u001b[0m, in \u001b[0;36mtrain_procedure\u001b[0;34m(run, epoch, log_base, mevis_args, merged_args)\u001b[0m\n\u001b[1;32m     77\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m lr_scheduler(\n\u001b[1;32m     78\u001b[0m     optimizer,\n\u001b[1;32m     79\u001b[0m     epoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     end_lr\u001b[38;5;241m=\u001b[39mmerged_args\u001b[38;5;241m.\u001b[39mlr_train_end,\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     86\u001b[0m dataset_train\u001b[38;5;241m.\u001b[39mon_epoch_end()\n\u001b[0;32m---> 87\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mevis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msam_mevis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlossfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_metric(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiceCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39mloss, step\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m     96\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalars(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiceCEloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss}, epoch)\n",
      "File \u001b[0;32m~/Thesis_code/retrain_v2.py:210\u001b[0m, in \u001b[0;36mtrain_mevis\u001b[0;34m(net, optimizer, loss_func, dataset, epoch, writer)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataset), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[0;32m--> 210\u001b[0m         pack \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    211\u001b[0m         masks \u001b[38;5;241m=\u001b[39m pack[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Train\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis_code/dataset_mevis_v2.py:357\u001b[0m, in \u001b[0;36mMRI_dataset_batched.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# Generate random click points\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueue\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 357\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_que_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_list[index]]\n",
      "File \u001b[0;32m~/Thesis_code/dataset_mevis_v2.py:122\u001b[0m, in \u001b[0;36mMRI_dataset_batched.get_que_batch\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmri index overshoot, calling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m start \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, end \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vol \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(volume_data):\n\u001b[0;32m--> 122\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_volume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    124\u001b[0m         loaded_batch \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/Thesis_code/dataset_mevis_v2.py:156\u001b[0m, in \u001b[0;36mMRI_dataset_batched._process_volume\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img_path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m    153\u001b[0m     img_vol \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    154\u001b[0m         sitk\u001b[38;5;241m.\u001b[39mGetArrayFromImage(sitk\u001b[38;5;241m.\u001b[39mReadImage(img_path)), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     img_vol \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_vol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_vol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_vol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_vol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading the image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if mevis_args.checkpoint_path != \"\":\n",
    "    new_path = Path(mevis_args.checkpoint_path)\n",
    "    if new_path.is_file() and new_path.suffix == \".pth\":\n",
    "        BONE_CHECKPOINT = new_path\n",
    "\n",
    "if \"bone_sam\" in BONE_CHECKPOINT.name:\n",
    "    epoch = 0\n",
    "    run_id = None\n",
    "else:\n",
    "    epoch = int(BONE_CHECKPOINT.name.split(\"epoch\")[1].split(\"-\")[0])\n",
    "    run_id = BONE_CHECKPOINT.name.split(\"-\")[-1]\n",
    "# run_id = \"putting_into_test\"\n",
    "\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n",
    "if run_id is not None:\n",
    "    with mlflow.start_run(run_id=run_id) as parent:\n",
    "        train_procedure(\n",
    "            parent,\n",
    "            epoch,\n",
    "            log_base=log_base,\n",
    "            mevis_args=mevis_args,\n",
    "            merged_args=merged_args,\n",
    "        )\n",
    "else:\n",
    "    with mlflow.start_run() as parent:\n",
    "        train_procedure(\n",
    "            parent,\n",
    "            epoch,\n",
    "            log_base=log_base,\n",
    "            mevis_args=mevis_args,\n",
    "            merged_args=merged_args,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run() as parent:\n",
    "    \n",
    "    base_save = CHECKPOINT_DIRECTORY / train_folder\n",
    "    base_save.mkdir(parents=True, exist_ok=True)\n",
    "    log_save = LOGDIR / train_folder\n",
    "    log_save.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(run_tags)\n",
    "\n",
    "    # data\n",
    "    dataset_validation = MRI_dataset_batched(\n",
    "        args,\n",
    "        data_file=VALID_DATA_FILE,\n",
    "        batch_size=mevis_args.batch_size,\n",
    "        phase=\"test\",\n",
    "        operation_mode=\"queue\",\n",
    "        mask_out_size=args.out_size,\n",
    "        attention_size=64,\n",
    "        crop=False,\n",
    "        crop_size=1024,\n",
    "        cls=1,\n",
    "        if_prompt=True,\n",
    "        prompt_type=\"point\",\n",
    "        if_attention_map=True,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    dataset_train = MRI_dataset_batched(\n",
    "        args,\n",
    "        data_file=TRAIN_DATA_FILE,\n",
    "        batch_size=mevis_args.batch_size,\n",
    "        phase=\"train\",\n",
    "        operation_mode=\"queue\",\n",
    "        mask_out_size=args.out_size,\n",
    "        attention_size=64,\n",
    "        crop=False,\n",
    "        crop_size=1024,\n",
    "        cls=1,\n",
    "        if_prompt=True,\n",
    "        prompt_type=\"point\",\n",
    "        if_attention_map=True,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    \n",
    "    # model\n",
    "    sam_mevis = build_sam_mevis(\n",
    "        args,\n",
    "        checkpoint=CHECKPOINT_DIRECTORY / \"mobile_sam.pt\",\n",
    "        mevis_checkpoint=CHECKPOINT_DIRECTORY / BONE_CHECKPOINT,\n",
    "        num_classes=2,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    for param in sam_mevis.parameters():\n",
    "        param.requires_grad = False\n",
    "    for n, m in sam_mevis.mask_decoder.named_parameters():\n",
    "        if \"Adapter\" in n:\n",
    "            m.requires_grad = True\n",
    "\n",
    "    total_trainable_params = sum(\n",
    "        p.numel() for p in sam_mevis.parameters() if p.requires_grad\n",
    "    )\n",
    "    mlflow.log_param(\"total_trainable_params\", total_trainable_params)\n",
    "    print(\"Number of trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "    lossfunc = DiceCELoss(sigmoid=True, squared_pred=True, reduction=\"mean\")\n",
    "    optimizer = torch.optim.AdamW(sam_mevis.parameters(), lr=mevis_args.warm)\n",
    "\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Training Info\", \"Basic LR model for iris data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sab_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
