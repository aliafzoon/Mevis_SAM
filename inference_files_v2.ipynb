{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=MIG-3b133eb8-ed94-51f8-937f-cbc3e3f3ff2a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MIG-3b133eb8-ed94-51f8-937f-cbc3e3f3ff2a'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=MIG-3b133eb8-ed94-51f8-937f-cbc3e3f3ff2a\n",
    "%env CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Thesis_code/models/sam/modeling/tiny_vit_sam.py:677: UserWarning: Overwriting tiny_vit_5m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/ubuntu/Thesis_code/models/sam/modeling/tiny_vit_sam.py:677: UserWarning: Overwriting tiny_vit_11m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/ubuntu/Thesis_code/models/sam/modeling/tiny_vit_sam.py:677: UserWarning: Overwriting tiny_vit_21m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/ubuntu/Thesis_code/models/sam/modeling/tiny_vit_sam.py:677: UserWarning: Overwriting tiny_vit_21m_384 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/ubuntu/Thesis_code/models/sam/modeling/tiny_vit_sam.py:677: UserWarning: Overwriting tiny_vit_21m_512 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.losses import DiceCELoss\n",
    "from tqdm.notebook import tqdm\n",
    "import SimpleITK as sitk\n",
    "\n",
    "import cfg\n",
    "from dataset_mevis_v2 import MRI_dataset_batched\n",
    "from dsc import dice_coeff\n",
    "from funcs import calculate_sensitivity_specificity\n",
    "from utils import iou_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFLOW_TRACKING_URI=http://0.0.0.0:5000\n"
     ]
    }
   ],
   "source": [
    "%env MLFLOW_TRACKING_URI=http://0.0.0.0:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mevis_args_parser():  \n",
    "    # changing prompt probability, do not import\n",
    "    parser = argparse.ArgumentParser(description=\"Mevis SAM fine-tuning parameters.\")\n",
    "    parser.add_argument(\n",
    "        \"--prompt_probability\",\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help=\"probability of generating prompts for each batch\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint_path\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Checkpoint to continue the training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_schedule\",\n",
    "        type=bool,\n",
    "        default=True,\n",
    "        help=\"Use earning rate scheduler during training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_train_start\",\n",
    "        type=float,\n",
    "        default=5e-4,\n",
    "        help=\"Learning rate starting value during training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_train_end\",\n",
    "        type=float,\n",
    "        default=5e-5,\n",
    "        help=\"Learning rate ending value during training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_warmup\", type=float, default=1e-5, help=\"Learning rate on warmup.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\", type=int, default=100, help=\"Number of training epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs_warmup\", type=int, default=20, help=\"Number of warmup epochs\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=120, help=\"Number of slices in a batch\"\n",
    "    )\n",
    "    mevis_args = parser.parse_args(\"\")\n",
    "\n",
    "    return mevis_args\n",
    "\n",
    "\n",
    "mevis_args = mevis_args_parser()\n",
    "args = cfg.parse_args()\n",
    "merged_args = argparse.Namespace(**vars(args), **vars(mevis_args))\n",
    "merged_args.prompt_probability = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_FILE = \"data_files/Test_data_files_resampled_v2.json\"\n",
    "TRAIN_DATA_FILE = \"data_files/Train_data_files_resampled_v2.json\"\n",
    "VALID_DATA_FILE = \"data_files/Validation_data_files_resampled_v2.json\"\n",
    "MODEL_NAME = \"mevis_sam_v2_epoch151\"\n",
    "MODEL_VERSION = \"1\"\n",
    "DEVICE = torch.device(\"cuda:\" + str(args.gpu_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = f\"models:/{MODEL_NAME}/{MODEL_VERSION}\"\n",
    "model = mlflow.pytorch.load_model(model_uri)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = MRI_dataset_batched(\n",
    "    merged_args,\n",
    "    data_file=TEST_DATA_FILE,\n",
    "    batch_size=mevis_args.batch_size,\n",
    "    phase=\"test\",\n",
    "    operation_mode=\"queue\",\n",
    "    mask_out_size=merged_args.out_size,\n",
    "    attention_size=64,\n",
    "    crop=False,\n",
    "    crop_size=1024,\n",
    "    cls=1,\n",
    "    if_prompt=True,\n",
    "    prompt_type=\"points\",\n",
    "    if_attention_map=True,\n",
    "    device=DEVICE,\n",
    ")\n",
    "dataset_validation = MRI_dataset_batched(\n",
    "    merged_args,\n",
    "    data_file=VALID_DATA_FILE,\n",
    "    batch_size=mevis_args.batch_size,\n",
    "    phase=\"test\",\n",
    "    operation_mode=\"queue\",\n",
    "    mask_out_size=merged_args.out_size,\n",
    "    attention_size=64,\n",
    "    crop=False,\n",
    "    crop_size=1024,\n",
    "    cls=1,\n",
    "    if_prompt=True,\n",
    "    prompt_type=\"points\",\n",
    "    if_attention_map=True,\n",
    "    device=DEVICE,\n",
    ")\n",
    "dataset_train = MRI_dataset_batched(\n",
    "    merged_args,\n",
    "    data_file=TRAIN_DATA_FILE,\n",
    "    batch_size=mevis_args.batch_size,\n",
    "    phase=\"test\",\n",
    "    operation_mode=\"queue\",\n",
    "    mask_out_size=merged_args.out_size,\n",
    "    attention_size=64,\n",
    "    crop=False,\n",
    "    crop_size=1024,\n",
    "    cls=1,\n",
    "    if_prompt=True,\n",
    "    prompt_type=\"points\",\n",
    "    if_attention_map=True,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7_t1.nii.gz', '7_t2_SPACE.nii.gz']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]\n"
     ]
    }
   ],
   "source": [
    "batch = dataset_test[0]\n",
    "print(batch[\"image_name\"])\n",
    "print(batch[\"slices\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_score_batch = dice_coeff(\n",
    "                    low_res_masks_bool[:, 1, :, :].to(torch.float32),\n",
    "                    batch[\"masks\"][:, 0, :, :].to(torch.float32),\n",
    "                ).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_make_roc(dataset, net, save_dir: Path, thresholds=[0]):\n",
    "    result_dict = {thresh: {} for thresh in thresholds}\n",
    "    with tqdm(\n",
    "        total=len(dataset) * len(thresholds),\n",
    "        desc=f\"Running inference on queued batches of size\",\n",
    "        unit=\"Batch\",\n",
    "    ) as pbar:\n",
    "        for ind in range(len(dataset)):\n",
    "            data = dataset[ind]\n",
    "            batch_size = data[\"images\"].shape[0]\n",
    "            pbar.set_description(\n",
    "                f\"Running inference on queued batches of size {batch_size}\"\n",
    "            )\n",
    "            pbar.refresh()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                low_res_masks = net.forward(\n",
    "                    data, multimask_output=True, if_attention=True\n",
    "                )\n",
    "            # low_res_masks_bool = low_res_masks > net.mask_threshold\n",
    "            for thresh in thresholds:\n",
    "                low_res_masks_bool = low_res_masks > thresh\n",
    "                cat_indexes = data.get(\"cat_indexes\", [0])\n",
    "                for j in range(len(cat_indexes)):\n",
    "                    orig_size = data[\"original_size\"][j]\n",
    "                    img_name = data[\"image_name\"][j]\n",
    "                    if len(cat_indexes) > j+1:\n",
    "                        slices = data[\"slices\"][cat_indexes[j] : cat_indexes[j + 1]]\n",
    "                        sliced_mask = data[\"masks\"][cat_indexes[j] : cat_indexes[j + 1]]\n",
    "                        sliced_prediction = low_res_masks_bool[\n",
    "                            cat_indexes[j] : cat_indexes[j + 1]\n",
    "                        ]\n",
    "                    else:\n",
    "                        slices = data[\"slices\"][cat_indexes[j] :]\n",
    "                        sliced_mask = data[\"masks\"][cat_indexes[j] :]\n",
    "                        sliced_prediction = low_res_masks_bool[cat_indexes[j] :]\n",
    "\n",
    "                    dice_score_img = dice_coeff(\n",
    "                        sliced_prediction[:, 1, :, :].to(torch.float32),\n",
    "                        sliced_mask[:, 0, :, :].to(torch.float32),\n",
    "                    ).item()\n",
    "                    with torch.no_grad():\n",
    "                        dice_ce = DiceCELoss(\n",
    "                            sigmoid=True, squared_pred=True, reduction=\"mean\"\n",
    "                        )(\n",
    "                            sliced_prediction[:, 1:2, :, :].to(torch.float32),\n",
    "                            sliced_mask.to(torch.float32),\n",
    "                        ).item()\n",
    "                    sensitivity, specificity = calculate_sensitivity_specificity(\n",
    "                        sliced_prediction[:, 1:2, :, :], sliced_mask, class_index=1\n",
    "                    )\n",
    "                    iou_batch = iou_torch(\n",
    "                        sliced_prediction[:, 1:2, :, :], sliced_mask.to(torch.int)\n",
    "                    )\n",
    "\n",
    "                    file_path = (  # not really saved. to indicate which slices are in\n",
    "                        save_dir\n",
    "                        / f\"{img_name.split(\".\")[0]}-{slices[0]}-{slices[-1]}.nrrd\"\n",
    "                    )\n",
    "                    if img_name in result_dict.keys():\n",
    "                        result_dict[thresh][img_name][\"prediction_path\"].append(\n",
    "                            str(file_path)\n",
    "                        )\n",
    "                        result_dict[thresh][img_name][\"sensitivity\"].append(sensitivity)\n",
    "                        result_dict[thresh][img_name][\"specificity\"].append(specificity)\n",
    "                        result_dict[thresh][img_name][\"iou\"].append(iou_batch)\n",
    "                        result_dict[thresh][img_name][\"dice_score\"].append(\n",
    "                            dice_score_img\n",
    "                        )\n",
    "                        result_dict[thresh][img_name][\"dice_ce\"].append(dice_ce)\n",
    "                    else:\n",
    "                        result_dict[thresh][img_name] = {\n",
    "                            \"prediction_path\": [str(file_path)],\n",
    "                            \"sensitivity\": [sensitivity],\n",
    "                            \"specificity\": [specificity],\n",
    "                            \"iou\": [iou_batch],\n",
    "                            \"dice_score\": [dice_score_img],\n",
    "                            \"dice_ce\": [dice_ce],\n",
    "                        }\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f634216122499b82de44e8fcc48aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size:   0%|          | 0/84 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f1b4c9e7fa49f1b7ce6f0803db6939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size:   0%|          | 0/84 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b4227b7d884bc485657e5f8bc86239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size:   0%|          | 0/658 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"ROC calculations\"\"\"\n",
    "\n",
    "model_identifier = \"mevis_sam-epoch151-29334c5d6e2b41bf86e613599ffeaff3\"\n",
    "eval_results_base = Path(\"./eval_results\")\n",
    "predicted_msk_folder = Path(\"/data/sab_data/predicted_masks/v2\")\n",
    "eval_results = eval_results_base / (model_identifier + \"/roc\")\n",
    "eval_results.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_dir = predicted_msk_folder / model_identifier\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "thresholds = np.arange(-5, 1.2, 1).tolist()\n",
    "\n",
    "test_inference = run_make_roc(\n",
    "    dataset_test, model, save_dir=save_dir, thresholds=thresholds\n",
    ")\n",
    "with open(\n",
    "    eval_results / f\"Test_inference_results_roc_{merged_args.prompt_probability}.json\",\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(\n",
    "        test_inference,\n",
    "        f,\n",
    "        indent=4,\n",
    "        sort_keys=False,\n",
    "        separators=(\",\", \": \"),\n",
    "    )\n",
    "\n",
    "validation_inference = run_make_roc(\n",
    "    dataset_validation, model, save_dir=save_dir, thresholds=thresholds\n",
    ")\n",
    "with open(\n",
    "    eval_results\n",
    "    / f\"Validation_inference_results_roc_{merged_args.prompt_probability}.json\",\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(\n",
    "        validation_inference,\n",
    "        f,\n",
    "        indent=4,\n",
    "        sort_keys=False,\n",
    "        separators=(\",\", \": \"),\n",
    "    )\n",
    "\n",
    "train_inference = run_make_roc(\n",
    "    dataset_train, model, save_dir=save_dir, thresholds=thresholds\n",
    ")\n",
    "with open(\n",
    "    eval_results / f\"Train_inference_results_roc_{merged_args.prompt_probability}.json\",\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(\n",
    "        train_inference,\n",
    "        f,\n",
    "        indent=4,\n",
    "        sort_keys=False,\n",
    "        separators=(\",\", \": \"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_save_inference(dataset, net, best_thresh: float, save_dir: Path, save=True):\n",
    "    # best_thresh = -1.0\n",
    "    result_dict = {}\n",
    "    with tqdm(\n",
    "        total=len(dataset),\n",
    "        desc=f\"Running inference on queued batches of size\",\n",
    "        unit=\"Batch\",\n",
    "    ) as pbar:\n",
    "        for ind in range(len(dataset)):\n",
    "            data = dataset[ind]\n",
    "            batch_size = data[\"images\"].shape[0]\n",
    "            pbar.set_description(\n",
    "                f\"Running inference on queued batches of size {batch_size}\"\n",
    "            )\n",
    "            pbar.refresh()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                low_res_masks = net.forward(\n",
    "                    data, multimask_output=True, if_attention=True\n",
    "                )\n",
    "            low_res_masks_bool = low_res_masks > best_thresh\n",
    "            cat_indexes = data.get(\"cat_indexes\", [0])\n",
    "            for j in range(len(cat_indexes)):\n",
    "                orig_size = data[\"original_size\"][j]\n",
    "                img_name = data[\"image_name\"][j]\n",
    "                if len(cat_indexes) > j+1:\n",
    "                    slices = data[\"slices\"][cat_indexes[j] : cat_indexes[j + 1]]\n",
    "                    sliced_mask = data[\"masks\"][cat_indexes[j] : cat_indexes[j + 1]]\n",
    "                    sliced_prediction = low_res_masks_bool[\n",
    "                        cat_indexes[j] : cat_indexes[j + 1]\n",
    "                    ]\n",
    "                else:\n",
    "                    slices = data[\"slices\"][cat_indexes[j] :]\n",
    "                    sliced_mask = data[\"masks\"][cat_indexes[j] :]\n",
    "                    sliced_prediction = low_res_masks_bool[cat_indexes[j] :]\n",
    "\n",
    "                dice_score_img = dice_coeff(\n",
    "                    sliced_prediction[:, 1, :, :].to(torch.float32),\n",
    "                    sliced_mask[:, 0, :, :].to(torch.float32),\n",
    "                ).item()\n",
    "                with torch.no_grad():\n",
    "                    dice_ce = DiceCELoss(\n",
    "                        sigmoid=True, squared_pred=True, reduction=\"mean\"\n",
    "                    )(\n",
    "                        sliced_prediction[:, 1:2, :, :].to(torch.float32),\n",
    "                        sliced_mask.to(torch.float32),\n",
    "                    ).item()\n",
    "                sensitivity, specificity = calculate_sensitivity_specificity(\n",
    "                    sliced_prediction[:, 1:2, :, :], sliced_mask, class_index=1\n",
    "                )\n",
    "                iou_batch = iou_torch(\n",
    "                    sliced_prediction[:, 1:2, :, :], sliced_mask.to(torch.int)\n",
    "                )\n",
    "\n",
    "                file_path = (  # not really saved. to indicate which slices are in\n",
    "                    save_dir\n",
    "                    / f\"{img_name.split(\".\")[0]}-{slices[0]}-{slices[-1]}.nrrd\"\n",
    "                )\n",
    "                orig_size_masks = net.postprocess_masks(\n",
    "                    masks=sliced_prediction.to(torch.float32),\n",
    "                    input_size=(1024, 1024),\n",
    "                    original_size=orig_size,\n",
    "                ).squeeze(1)\n",
    "\n",
    "                if img_name in result_dict.keys():\n",
    "                    result_dict[img_name][\"prediction_path\"].append(str(file_path))\n",
    "                    result_dict[img_name][\"sensitivity\"].append(sensitivity)\n",
    "                    result_dict[img_name][\"specificity\"].append(specificity)\n",
    "                    result_dict[img_name][\"iou\"].append(iou_batch)\n",
    "                    result_dict[img_name][\"dice_score\"].append(dice_score_img)\n",
    "                    result_dict[img_name][\"dice_ce\"].append(dice_ce)\n",
    "                else:\n",
    "                    result_dict[img_name] = {\n",
    "                        \"prediction_path\": [str(file_path)],\n",
    "                        \"sensitivity\": [sensitivity],\n",
    "                        \"specificity\": [specificity],\n",
    "                        \"iou\": [iou_batch],\n",
    "                        \"dice_score\": [dice_score_img],\n",
    "                        \"dice_ce\": [dice_ce],\n",
    "                    }\n",
    "                if save:\n",
    "                    nrrd_vol = sitk.GetImageFromArray(\n",
    "                        orig_size_masks.to(torch.uint8).cpu().numpy()[:, 1, :, :]\n",
    "                    )\n",
    "                    sitk.WriteImage(nrrd_vol, fileName=file_path)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344a0e57859a4261a2b0b00449133b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size:   0%|          | 0/12 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae82fbecde6c4bcabbb6c0fda2ec920b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size:   0%|          | 0/12 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537dde57a76b48bdba5ab59e11e29d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inference on queued batches of size:   0%|          | 0/94 [00:00<?, ?Batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_thresh = -1.0\n",
    "model_identifier = \"mevis_sam-epoch151-29334c5d6e2b41bf86e613599ffeaff3\"\n",
    "eval_results_base = Path(\"./eval_results\")\n",
    "predicted_msk_folder = Path(\"/data/sab_data/predicted_masks/v2\")\n",
    "eval_results = eval_results_base / model_identifier\n",
    "eval_results.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_dir = predicted_msk_folder / model_identifier\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "save = True\n",
    "\n",
    "test_inference = run_save_inference(\n",
    "    dataset_test, model, best_thresh=best_thresh, save_dir=save_dir, save=save\n",
    ")\n",
    "with open(\n",
    "    eval_results\n",
    "    / f\"Test_inference_results_{merged_args.prompt_probability}_thresh{best_thresh}.json\",\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(\n",
    "        test_inference,\n",
    "        f,\n",
    "        indent=4,\n",
    "        sort_keys=False,\n",
    "        separators=(\",\", \": \"),\n",
    "    )\n",
    "\n",
    "validation_inference = run_save_inference(\n",
    "    dataset_validation, model, best_thresh=best_thresh, save_dir=save_dir, save=save\n",
    ")\n",
    "with open(\n",
    "    eval_results\n",
    "    / f\"Validation_inference_results_{merged_args.prompt_probability}_thresh{best_thresh}.json\",\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(\n",
    "        validation_inference,\n",
    "        f,\n",
    "        indent=4,\n",
    "        sort_keys=False,\n",
    "        separators=(\",\", \": \"),\n",
    "    )\n",
    "\n",
    "train_inference = run_save_inference(\n",
    "    dataset_train, model, best_thresh=best_thresh, save_dir=save_dir, save=save\n",
    ")\n",
    "with open(\n",
    "    eval_results\n",
    "    / f\"Train_inference_results_{merged_args.prompt_probability}_thresh{best_thresh}.json\",\n",
    "    \"w\",\n",
    ") as f:\n",
    "    json.dump(\n",
    "        train_inference,\n",
    "        f,\n",
    "        indent=4,\n",
    "        sort_keys=False,\n",
    "        separators=(\",\", \": \"),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sab_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
